<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>datapythonista blog - pandas</title><link href="https://datapythonista.github.io/blog/" rel="alternate"></link><link href="https://datapythonista.github.io/blog/feeds/pandas.atom.xml" rel="self"></link><id>https://datapythonista.github.io/blog/</id><updated>2019-11-28T00:00:00+00:00</updated><subtitle>about me</subtitle><entry><title>An update on the pandas documentation</title><link href="https://datapythonista.github.io/blog/pandas-documentation-update.html" rel="alternate"></link><published>2019-11-28T00:00:00+00:00</published><updated>2019-11-28T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2019-11-28:/blog/pandas-documentation-update.html</id><summary type="html">&lt;h2&gt;Some context&lt;/h2&gt;
&lt;p&gt;This post is mainly a technical post on what's the status of the pandas documentation.
But let me provide a bit of context on where this comes from.&lt;/p&gt;
&lt;p&gt;It's a personal opinion, but I think pandas is one of the clearest examples of how open
source is transforming …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Some context&lt;/h2&gt;
&lt;p&gt;This post is mainly a technical post on what's the status of the pandas documentation.
But let me provide a bit of context on where this comes from.&lt;/p&gt;
&lt;p&gt;It's a personal opinion, but I think pandas is one of the clearest examples of how open
source is transforming the data world (together with some other projects like scikit-learn,
Jupyter, R...). Is likely that pandas contributed to the huge growth of
Python, a language that not many people knew about when pandas development started.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/panda_book.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;And I think its documentation has been the clearest example of the paradox of open
source software. While pandas users were growing to the millions, and it was adopted in thousands
of companies (including the largest companies in the world), almost nobody spent any time
or money in its documentation (which requires a lot of work to be of high quality). It's surely not to blame the
very few people (3 or 4 at times) who were maintaining the project. They had enough dealing with thousands of
issues, updates in the many and fast changing dependencies, releasing new versions... While also
implementing new features and fixing bugs themselves. And even more considering that most of that
work was done as volunteers, in evenings after work or in weekends.&lt;/p&gt;
&lt;p&gt;More than 2 years ago I sent to the project my first pull request, fixing a single docstring
(one of the around 2,000 in the project). And decided to spend a significant amount of
my also volunteer time (after work and during weekends) in improving that part. That also
was one of the main reasons for starting the &lt;a href="https://python-sprints.github.io"&gt;Python sprints&lt;/a&gt;
group.&lt;/p&gt;
&lt;p&gt;More than two years later, not much changed (apparently). But if you care about my opinion,
I'm sure very soon pandas will have one of the best documentations of any open source project. This
post explains all the work done by hundreds of people in the last couple of years, and the work
that is still missing, and how we are going to approach it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you work for a company that is making money using pandas, and that would be more productive
and make even more money if pandas documentation was better, please contact a pandas maintainer
including &lt;a href="mailto:garcia.marc@gmail.com"&gt;myself&lt;/a&gt; or &lt;a href="https://www.numfocus.org"&gt;NumFOCUS&lt;/a&gt;.
We are happy to discuss funding opportunities, including small grants or helping your company
hire people to work on pandas.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The problem with the docstrings&lt;/h2&gt;
&lt;p&gt;The pandas API is huge, and includes around 2,000 functions, methods, classes, attributes...
Each of them with a page in the documentation.
Given the very reduced number of developers pandas had, and the huge demand and urgency for a dataframe
library in Python, most of those API pages couldn't be created with high standards when the
features were implemented. See for example the
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.resample.Resampler.last.html"&gt;Resampler.last&lt;/a&gt;
page.&lt;/p&gt;
&lt;p&gt;It may not be obvious for people who haven't contributed to a project like pandas before,
but improving a docstring, and make it as useful for readers as possible, it's not 5 minutes
of work. Based on the work done by lots of contributors over the last year, I would estimate
it takes around 1 day of work of an experienced pandas user. You can see the docstring of
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.resample.Resampler.bfill.html"&gt;Resampler.bfill&lt;/a&gt;
to see what I'd consider a good docstring.&lt;/p&gt;
&lt;p&gt;Considering this estimate of 1 day per docstring, and the around 2,000 docstrings, it's
easy to calculate that it'd take around 8 years for a single person working full time,
to have all them fixed. And that excludes the time of maintainers to review the changes,
provide feedback, merge...&lt;/p&gt;
&lt;h2&gt;The pandas documentation sprint&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/pandas_sprint.png"&gt;&lt;/p&gt;
&lt;p&gt;As it's obvious that a single person can't do much, one of the things that was tried
was to organize a &lt;a href="https://python-sprints.github.io/pandas/"&gt;worldwide pandas sprint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The sprint was extremely successful in many ways. 30 local users groups participated,
from places as diverse as Korea, Hong Kong, India, Turkey, Kenya, Nigeria, Argentina
or Brazil, besides many cities in Europe and US. Difficult to say how many people
participated, but we estimate there where around 500 people helping make pandas
documentation for a whole Saturday.&lt;/p&gt;
&lt;p&gt;I think there are no words to describe how amazing that is. How many people offered
to organize sprints with their local groups. And how many people joined them in every
sprint. The organizers had to prepare the event for weeks, both for logistics but also
for the technical part of leading a lot of people in their first open source contributing.&lt;/p&gt;
&lt;p&gt;A big responsible for this success was &lt;a href="https://numfocus.org/"&gt;NumFOCUS&lt;/a&gt;. For several
years now NumFOCUS has done extraordinary efforts on building the PyData community.
Having a connected network of more than 150 user groups made it very easy to communicate
and reach all the people who could be interested.&lt;/p&gt;
&lt;p&gt;The feedback I received from the participants and organizers was very positive, and people
enjoyed the experience (which I personally think it's much more important than the
contributions made). And there were around 200 documentation pages that were fixed
because of the sprint (10% of the total).&lt;/p&gt;
&lt;p&gt;But not everything was so positive. The sprint also made evident that the problem of
fixing the documentation was not the number of contributors. The bottleneck happened
to be the maintainers. The sprint created a total disruption of the project for more than two
weeks. And this period wasn't longer because the maintainers were spending day and night
reviewing the pull requests from the sprint. In many cases it was the maintainers who
had to finish the work started during the sprint. Many pull requests contained great
work, but were discontinued, and they required important changes that had to be done
by the maintainers. The last PR from the sprint was merged almost a year later than
the sprint.&lt;/p&gt;
&lt;p&gt;More information about the sprint can be found in this
&lt;a href="https://numfocus.org/blog/worldwide-pandas-sprint"&gt;NumFOCUS write up&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;The validation script&lt;/h2&gt;
&lt;p&gt;We anticipated before the sprint, that reviewing the contributions would be a lot
of work. And we developed a script to automate part of it. The idea was to automatically
detect things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The documented parameters of a function need to match the actual parameters in the signature&lt;/li&gt;
&lt;li&gt;Conventions like starting sentences with a capital letter or finishing with a period&lt;/li&gt;
&lt;li&gt;Ensure that some sections we'd like to always have are present (like Examples)&lt;/li&gt;
&lt;li&gt;Formats that make Sphinx render the documentation incorrectly, like missing spaces before the
  colon between a parameter name and its type&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We managed to have several of these ready for the sprint. But to keep all the docstrings consistent
and rendering correctly, we needed many more. Many people spent a significant amount of time adding
new checks to the script, and we are currently able to automatically detect
&lt;a href="https://github.com/pandas-dev/pandas/blob/master/scripts/validate_docstrings.py#L63"&gt;more than 40 possible problems in docstrings&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course not everything can be validated, think of correct grammar, wrong, inaccurate or misleading
information, examples that are not clear... But validating almost every formatting issue automatically
will definitely save a lot of time of reviewers. Who will be able to focus on the things that
can't be automated.&lt;/p&gt;
&lt;p&gt;Some other projects had interest in using our validation script, so it's been recently moved to
&lt;a href="https://github.com/numpy/numpydoc"&gt;numpydoc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/numpydoc_validation.png"&gt;&lt;/p&gt;
&lt;h2&gt;Continuous Integration&lt;/h2&gt;
&lt;p&gt;During the sprint, we provided clear instructions, and we had mentors in each of the 30 different
locations. So, everybody was probably aware that the validation script was one of the main things
they had to check. But regular contributors don't get this sort of induction. So, ideally we would
like to run the checks automatically in the CI, so contributors are aware of any problem in their
proposed changes. But there are some things to consider.&lt;/p&gt;
&lt;p&gt;With Travis, our main CI system, the errors ended up in a huge log, that only experienced
pandas developers are able to understand. See for example &lt;a href="https://travis-ci.org/pandas-dev/pandas/jobs/484898115"&gt;this job&lt;/a&gt;
and make sure you wait until it's fully loaded. :)&lt;/p&gt;
&lt;p&gt;Luckily, at the time our script was being implemented, Numba set up Azure Pipelines for their CI, and we
decided to use it to complement Travis. The main reason was that we required more computational
power for the huge test set of pandas, and the large number of builds. But, I think the clarity
on how the errors can be reported, is as convenient as the 30 concurrent jobs the Azure team
offered us. Compare this with the previous Travis log:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://user-images.githubusercontent.com/10058240/47961709-1ca90800-e008-11e8-80d7-cccc2c2e5776.png"&gt;&lt;/p&gt;
&lt;p&gt;We were very ambitious, and somehow pioneers in what we were doing, and it wasn't
easy to get what we wanted. But The Azure team was extremely helpful, and the final
presentation of errors in docstrings, as well as the linting errors and others was
much clearer. And friendly for first time contributors.&lt;/p&gt;
&lt;p&gt;Azure pipelines was an improvement, but we still had some challenges. The integration
with GitHub wasn't great, and it required following two links and navigating a somehow
confusing interface to arrive to the page where the errors were being presented in a
better interface than TravisCI.&lt;/p&gt;
&lt;p&gt;Just few days ago we moved this docstring validation, among other things, to
&lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt;. So far the experience has been
great, and we keep the same advantages as with Pipelines, and also we have a better
integration of the CI with GitHub. And the configuration is also simpler and a bit
more intuitive than with Pipelines.&lt;/p&gt;
&lt;p&gt;We are still figuring out how to notify contributors with a human-readable message
about problems in the CI. Since only experienced contributors know how to find the
problems by themselves. And that's taking time from the reviewers, for a task that
could be easily automated. With GitHub Actions is very easy to write comments in
a pull request, so the only remaining challenge is to obtain all the failures in
all the jobs in a run. But hopefully that's not too complex and we can have it in
place soon.&lt;/p&gt;
&lt;p&gt;Another important thing that the continuos integration can do for us, is to let
us visualize the documentation before being merged. In many cases, small details
make the documentation render in an incorrect way. Like a bold style that doesn't
finish when it's expected to finish. Or a broken table that renders as plain text.&lt;/p&gt;
&lt;p&gt;This has been challenging until now, since CI systems don't do this automatically,
and it's tricky to implement manually. CircleCI provided this feature, but to
access the rendered documentation you have to know the url and replace the id of
your build on it. Maintainers could do that in a not very efficient way, but
first-time contributors couldn't guess.&lt;/p&gt;
&lt;p&gt;Implementing it manually has a main challenge. The CI is usually only able to
access passwords and keys when a pull request is merged. Otherwise a malicious
pull request could access the key and leak it. An option to overcome this limitation
would be to host in a server a GitHub application that receives a webhook when
a pull request is opened or updated, and its docs are built. After being triggered
it would fetch the docs, and publish them somewhere, using a key only accessible
in the server.&lt;/p&gt;
&lt;p&gt;Implementing that has been in our backlog for a while. But there is another problem.
Where to host all these copies of the documentation. We use GitHub pages to
host the latest version of our docs. But since Git/GitHub track all the history
of changes, the repo would grow huge very quickly if we add all our docs (around
50Mb) for every pull request.&lt;/p&gt;
&lt;p&gt;The solution to this may come easier than expected, since GitHub Actions seems to be consideting
implementing it. Not sure when that could be ready, if ever. In the meantime &lt;a href="https://www.ovh.co.uk/"&gt;OVH&lt;/a&gt;
has recently started sponsoring a new pandas cloud hosting, were we can temporary store
all these versions of the docs. This is being implemented now, and with some
luck should be in production soon. If we implement our GitHub app to publish the docs, the
new hosting and GitHub Actions will make it much easier than it was before.
GitHub Actions will also
make very simple to delete the docs for a PR once it is merged or closed.&lt;/p&gt;
&lt;h2&gt;Validating docstrings in the CI&lt;/h2&gt;
&lt;p&gt;Two key pieces to validate contributions to the pandas docstrings are in place:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A validation script with lots of checks&lt;/li&gt;
&lt;li&gt;A CI system friendly with first time contributors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But there is a last piece needed. If we activate the validation in the CI, we will
have 1,000 docstrings, or more, reporting errors in the CI for every PR. Basically,
we will be reported of every error in every docstring that needs to be fixed every
time the CI runs.&lt;/p&gt;
&lt;p&gt;Would be useful to be able to get the errors being introduced, so no more errors
are added, while we fix the ones we already have. But this is not feasible (would be
too complex and too slow).&lt;/p&gt;
&lt;p&gt;This leaves us in a unfortunate position, of only being able to validate what has
already been fixed. Which is extremely useful, as we can guarantee that things don't
get worse. But it doesn't solve our problem of improving the docstrings that need it yet.&lt;/p&gt;
&lt;p&gt;So, what can be done? I think it's useful to divide the docstring checks in two
different categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pure formatting things (like having periods at the end of sentences)&lt;/li&gt;
&lt;li&gt;The ones that require knowledge of pandas and the object being documented
  (like adding examples of how to use an function, or adding an undocumented parameter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The ones in the first category are somehow easy to fix. And many can be fixes at a time
(in a single pull request), working in one after the other, without stopping much to
understand things.&lt;/p&gt;
&lt;p&gt;Once all the errors of a kind have been fixed, we can start validating that specific
error in the CI. And we guarantee that no more errors of that kind will ever happen.&lt;/p&gt;
&lt;p&gt;And what is even better. Once all those formatting errors are fixed, we can start
working on the docstrings, one at a time. Where a person spends enough time understanding
the function being documented to become an expert. To a point where it can focus on
improving the content, writing a nice summary, useful examples, and document parameters
in an accurate way. And that person won't need to become an expert in reStructuredText
formatting, because the CI warn about any issue. And the person will be able to fix
any problem without requiring time from a maintainer.&lt;/p&gt;
&lt;p&gt;We already completely fixed several of the more than 40, and we validate that the errors are
not added again:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GL03&lt;/strong&gt;: Use of double blank lines when one would be expected&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL04&lt;/strong&gt;: Private classes mentioned in public docstrings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL05&lt;/strong&gt;: Tabs used instead of 4 spaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL06&lt;/strong&gt;: An unknown section is found&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL07&lt;/strong&gt;: Sections in the wrong order&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL09&lt;/strong&gt;: Deprecation warning in the wrong position&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL10&lt;/strong&gt;: Sphinx directives incorrectly formatted&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SS04&lt;/strong&gt;: Summary contains heading whitespaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SS05&lt;/strong&gt;: Summary staring with infinitive verb, not third-person&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR03&lt;/strong&gt;: Parameters are in the wrong order (compared to the signature)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR04&lt;/strong&gt;: Parameters without type&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR05&lt;/strong&gt;: Parameter type finishing with a period (it shouldn't)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR10&lt;/strong&gt;: Missing space before colon splitting parameter name and type&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RT01&lt;/strong&gt;: No Returns section&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RT04&lt;/strong&gt;: Returns description should start with a capital letter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RT05&lt;/strong&gt;: Returns description should finish with a period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA01&lt;/strong&gt;: No See Also section found&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA02&lt;/strong&gt;: See Also description should finish with a period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA03&lt;/strong&gt;: See Also description should start with a capital letter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA05&lt;/strong&gt;: Unneeded prefix in See Also section object&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EX04&lt;/strong&gt;: pandas or NumPy explicitly imported in the examples (we assume they are always imported)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those validations have been added to the &lt;a href="https://github.com/pandas-dev/pandas/blob/master/ci/code_checks.sh#L289"&gt;code_checks.sh&lt;/a&gt;
script, which is responsible for many code quality checks (linting, typing, avoidance of unwanted patterns...).&lt;/p&gt;
&lt;p&gt;But there are still many errors that need to be fixed, including formatting errors.&lt;/p&gt;
&lt;p&gt;We are able to get the list of all the errors that we can automatically detect,
with the same script that analyzes a docstring, but without passing an object as a parameter::&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./scripts/validate_docstrings.py --format&lt;span class="o"&gt;=&lt;/span&gt;json &amp;gt; docstrings.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will output a JSON file that can easily be loaded into pandas,
and see what needs to be fixed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;docstrings.json&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orient&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;errors&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;err_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;err&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;err_list&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;barh&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/pandas_docstring_errors.png"&gt;&lt;/p&gt;
&lt;p&gt;We can see how there are 18 different validations with docstrings that don't pass them.&lt;/p&gt;
&lt;p&gt;The list of all the validations and their error codes can be found in the same
&lt;a href="https://github.com/pandas-dev/pandas/blob/master/scripts/validate_docstrings.py#L77"&gt;validation script code&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can see for example, how the error &lt;code&gt;GL08&lt;/code&gt;, happening more than 500 times, means that
the objects don't have a docstring at all. See for example
&lt;a href="http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.empty.html"&gt;Series.empty&lt;/a&gt;
Or the second most frequent error, &lt;code&gt;RT03&lt;/code&gt;
means that it is not documented what is being returned (the return type is present, but
there is no description on what's the returned object).&lt;/p&gt;
&lt;p&gt;In total we have 2,441 errors that we can detect automatically. That includes docstrings that
need to be fully created. And creating them requires a lot of time, since there is no documentation
of them, and analyzing the source code, experimenting... is required.&lt;/p&gt;
&lt;h2&gt;Where do we come from?&lt;/h2&gt;
&lt;p&gt;Analyzing the docstrings of past versions of pandas we can see that pandas 0.23 had
7,136 errors, with the current validations. pandas 0.23 was released on the 16th of
May of 2018, couple of month after the worldwide sprint with 500 people. Most of the
work of that sprint was already present in that version.&lt;/p&gt;
&lt;p&gt;This is the detail of errors at that time:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/pandas_docstring_errors_023.png"&gt;&lt;/p&gt;
&lt;p&gt;So, in 18 months we fixed two thirds of the errors. Assuming it takes the same amount
of time to fix every error, and that we continue at the same pace, in 9 months more we
should have all them fixed. This would amazing, but it's clearly false that all errors
require the same time, and it's obvious that we started by the easy ones. But we are
getting closer.&lt;/p&gt;
&lt;h2&gt;Parallelization, a game changer&lt;/h2&gt;
&lt;p&gt;Our work is every time more challenging and time consuming. Remember that we started with
small formatting changes like adding a period at the end of a sentence, and we are going 
to write full docstrings. So, we need to become smarter and more efficient on how we work on the
documentation. And the most important part is being able to work in parallel, since there is
not much a single person can do.&lt;/p&gt;
&lt;p&gt;Of the around 2,000 docstrings, around 1,400 still have errors (considering errors detected
by the script). Imagine that tomorrow we want to start working on all them. One of the main
problems would be to detect who works on what, and avoid duplicate work. Many people volunteered
in the past to help make the pandas documentation better, and there may be 1,400 people
out there who could be happy to help if they knew exactly what to do.&lt;/p&gt;
&lt;p&gt;How this is usually solved in open source is by creating issues. Then people assigned the
issue they want to work to themselves, and as people join the party, they check in the
list of unassigned issues what else need to be done.&lt;/p&gt;
&lt;p&gt;Unfortunately, a workflow as simple as this has been problematic in GitHub. While GitHub
provides a nice interface for issues, and implements the functionality of assignees, it's
limited to members of the organization (core developers). While this is useful for small
corporate projects, assignees is unused in pandas, and people have been assigning them
issues by mentioning their interest in a comment. Which it kind of works, but it doesn't
let people find unassigned issues in an easy way.&lt;/p&gt;
&lt;p&gt;Recently, we found a hack in pandas, that people is starting to use. With GitHub actions,
we are able to assign issues to people who adds a comment with the exact word `take.
While this is far from optimal, since most users won't be aware of this, it can be very
useful when organizing sprints or coordinated events, where we can communicate with
contributors. We can also write in the issue description about this new feature, so
creating a batch of issues automatically, that we expect people to assign to themselves
seems feasible.&lt;/p&gt;
&lt;p&gt;That's a game changer in parallelizing the work, since it makes the logistics much
simpler in coordinating the contributors. See more information about this new
workflow in this &lt;a href="https://datapythonista.github.io/blog/new-pandas-workflow.html"&gt;recent blog post&lt;/a&gt;
I wrote.&lt;/p&gt;
&lt;p&gt;But there are still some challenges:&lt;/p&gt;
&lt;p&gt;In many cases, pandas has templates for a docstring, that are reused in more
than one object. So different objects can use the same docstring, and it's not
easy to identify which. Creating issues automatically for each object can still
cause overlap in the work.&lt;/p&gt;
&lt;p&gt;While many things can be automated, the main bottleneck are still the reviews of
the changes. There are few people who has the knowledge and experience in pandas
to provide feedback on whether the changes to the documentation are reasonable.
And who has the permissions to merge changes into pandas. And almost all them
are volunteer, and need to take care of other obligations like full time jobs
and family.&lt;/p&gt;
&lt;h2&gt;The future&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/panda_babies.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Exciting times are coming for the pandas documentation. Not only the content, but
also a whole new theme is being implemented. So, soon the documentation will
look much better.&lt;/p&gt;
&lt;p&gt;The CI is getting closer to a level where we can automate as much as possible,
and be very efficient in coordinating the documentation efforts. As well as
let first-time contributors be quite autonomous, and progress independently on
their work until a maintainer can really add value.&lt;/p&gt;
&lt;p&gt;Not only the number of maintainers in the project has been growing significantly
in the last couple of years, but a new role of triaggers has been implemented,
which can be useful in this effor.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://numfocus.org/"&gt;NumFOCUS&lt;/a&gt; has recently awarded us a small development
grant to work on the documentation. And not only will help with the documentation
but hopefully will address a problem as big as it, diversity. A group of people
from groups underrepresented as pandas contributors will be helping with the
documentation, and organizing sprints in different locations to help and
encourage more people to contribute to it.&lt;/p&gt;
&lt;p&gt;Increasing the diversity of the background of the contributors, will not only
help with the quantity of the documentation pages, but also its quantity.
pandas should be useful for a wide variety of people. If the documentation is
made and reviewed by people from different backgrounds (academic backgrounds,
geography, gender...) it will be clearer and more useful to more people, and
will better accomplish its goal.&lt;/p&gt;
&lt;p&gt;The sprints are likely to be in Jakarta, Cairo and Berlin. If you can increase
the diversity of the pandas contributors and want to participate, feel free
to contact me for more details and updates.&lt;/p&gt;
&lt;p&gt;If you would like to organize a pandas documentation sprint for minorities in
a different location, please also get in touch. We are unlikely to be able to
provide funds, but we can help you with everything else.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And if your company would benefit from a better pandas documentation, please
consider supporting the project. From funding to the project, to funding of
specific developments. And also you can consider hosting an event in your
office, letting your employees spend part of their time working on pandas,
providing in-kind donations, or anything you can think of. Please message
me or any other maintainer to discuss about opportunities.&lt;/strong&gt;&lt;/p&gt;</content><category term="pandas"></category></entry><entry><title>New pandas workflow</title><link href="https://datapythonista.github.io/blog/new-pandas-workflow.html" rel="alternate"></link><published>2019-11-17T00:00:00+00:00</published><updated>2019-11-17T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2019-11-17:/blog/new-pandas-workflow.html</id><summary type="html">&lt;p&gt;Some exciting news. After some years of organizing &lt;a href="https://python-sprints.github.io/"&gt;sprints&lt;/a&gt;,
and maintaining open source, I've been thinking on a more efficient workflow for projects with
high volume of activity, like &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An exaggerated example would be that I want to create 1,600 issues in pandas. One for each
docstring of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Some exciting news. After some years of organizing &lt;a href="https://python-sprints.github.io/"&gt;sprints&lt;/a&gt;,
and maintaining open source, I've been thinking on a more efficient workflow for projects with
high volume of activity, like &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An exaggerated example would be that I want to create 1,600 issues in pandas. One for each
docstring of the project, with the flaws that we are able to automatically detect. As a
side know, most of our validations to detect incorrect things in docstrings based on the
&lt;a href="https://numpydoc.readthedocs.io/en/latest/format.html"&gt;numpydoc standard&lt;/a&gt; are now available
in &lt;code&gt;numpydoc&lt;/code&gt; (in master). You can check the
&lt;a href="https://numpydoc.readthedocs.io/en/latest/validation.html"&gt;documentation&lt;/a&gt; to
see how to use it. And the &lt;a href="https://github.com/numpy/numpydoc/blob/master/numpydoc/validate.py#L35"&gt;source code&lt;/a&gt;
for the list of errors we validate.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/numpydoc_validation.png"&gt;&lt;/p&gt;
&lt;p&gt;Back to the example, &lt;a href="https://developer.github.com/v3/"&gt;GitHub API&lt;/a&gt; and our validations
scripts would make it very easy to create those 1,600 GitHub issues. We could create a
label &lt;code&gt;Docstring errors&lt;/code&gt; to identify them, and ask the community for help to fix
those. The community responded extremely well in the past when we ask them for help.
500 people joined our &lt;a href="https://numfocus.org/blog/worldwide-pandas-sprint"&gt;worldwide documentation sprint&lt;/a&gt;.
So, things seem feasible so far.&lt;/p&gt;
&lt;p&gt;There are just two main problems to make all this work:&lt;/p&gt;
&lt;p&gt;First, there is a small number of maintainers who would have to review, give feedback, and
merge the contributions. 1,600 pull requests is surely too much for a small group
of volunteers. We are surely in a much better position now, than when 500 people
contributed in a single day (it took months to deal with all the pull requests of the sprints).
We are around 12 active maintainers, compared to 4 at that time.
And we've been improving on making our workflow more efficient, with the
CI providing every time better feedback. More accurate, and presented in a better
way, so first time contributors can detect problems in their work without much
intervention from maintainers. &lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt;
will be key in making our workflow more efficient for code reviews (things like
contributors receiving automated emails when the CI detects something that needs to
be fixed in their work).&lt;/p&gt;
&lt;p&gt;Second, how could people know which of the 1,600 issues are available, and which
are already in the works by someone else? For small projects, GitHub has an option
&lt;code&gt;Assignees&lt;/code&gt; where members of a scrum team can assign to themselves what they are
working on. But this is not possible for a project the size of pandas, since only members
of the organization are able to self-assign issues. And even if we wanted to add
every possible contributor to the pandas GitHub organization, that would be a huge
amount of work for maintainers.&lt;/p&gt;
&lt;p&gt;The best solution should come from GitHub. Adding an option so project admins can
decide whether they want to allow any GitHub user to self-assign issues in their
projects. I've been discussing this with people at GitHub, and it is something it
may be added. But not immediately.&lt;/p&gt;
&lt;p&gt;The good news is that with the help of &lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt;
is now possible to achieve the same, in a slightly trickier way. We just added
to pandas an &lt;a href="https://github.com/pandas-dev/pandas/pull/29648"&gt;action to self-assign issues&lt;/a&gt;.
How it works is by just writing a comment with the keyword &lt;code&gt;take&lt;/code&gt; to an issue.
And few seconds later, the action will assign the issue to the commenter. This
is possible because few months ago GitHub added a feature to let
&lt;a href="https://github.blog/2019-06-25-assign-issues-to-issue-commenters/"&gt;assign issues to issue commenters&lt;/a&gt;.
It is not possible even for maintainers to assign an issue to an arbitrary user.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/github_actions_assign.png"&gt;&lt;/p&gt;
&lt;p&gt;With this simple but powerful change, now a much more efficient workflow should
be possible. The workflow could consist in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;People interested in contributing to pandas start by &lt;a href="https://python-sprints.github.io/pandas/guide/pandas_setup.html"&gt;setting up the environment&lt;/a&gt;
  and learn &lt;a href="https://docs.google.com/presentation/d/1rOSYXZPyMe9KXnbVK_xbJzw_-ijxd6bIxndmvPU6L2o/edit?usp=sharing"&gt;how to make an open source contribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Then they check the list of &lt;a href="https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22+no%3Aassignee"&gt;unassigned good first issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Once they find one that they want to work on, they write a comment with the keyword &lt;code&gt;take&lt;/code&gt; on it&lt;/li&gt;
&lt;li&gt;The issue will disappear from the list of unassigned issues,
  other people won't waste time checking whether it's available or not&lt;/li&gt;
&lt;li&gt;If the person can't finally move forward (got busy, they are not interested anymore...)
  they can simply unassign themselves from the issue, and it will be in the list again&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This new workflow scales to the 1,600 issues or more. Before, potential contributors had
a list with all issues, assigned and not assigned. They had to check each individually for comments
claiming the issue, deal with ambiguity (do messages like "can I work on this?" mean you're working
on the issue?), and possibly have some discussion, before they could know if someone else is working in the issue.&lt;/p&gt;
&lt;p&gt;One obvious problem is if people self-assigning an issue, discontinuing work on it, but not
unassigning the issue. We will see how this works, but even in the worst case, unassigned
issues will still be easy to find if they exist. For the assigned ones, people can check them,
and know immediately who to ask to know if work is still going on, or progress was made.
And to ask if the original assignee is happy to hand over the issue to the new interested contributor.&lt;/p&gt;
&lt;p&gt;Implementing a bot that unassignes issues automatically after N days of inactivity could
also be an option.&lt;/p&gt;</content><category term="pandas"></category></entry><entry><title>Dataframe summit @ EuroSciPy write up</title><link href="https://datapythonista.github.io/blog/dataframe-summit-at-euroscipy.html" rel="alternate"></link><published>2019-09-11T00:00:00+01:00</published><updated>2019-09-11T00:00:00+01:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2019-09-11:/blog/dataframe-summit-at-euroscipy.html</id><summary type="html">&lt;p&gt;Last week took place in Bilbao, Spain, &lt;a href="https://www.euroscipy.org/2019/"&gt;EuroSciPy 2019&lt;/a&gt;.
This year we introduced the &lt;a href="https://www.euroscipy.org/2019/maintainers.html"&gt;maintainers track&lt;/a&gt;
a room dedicated to discussions among maintainers. The idea is similar to the 
&lt;a href="https://en.wikipedia.org/wiki/Birds_of_a_feather_(computing)"&gt;birds of a feather&lt;/a&gt; or unconference
sessions of other conferences. But focussed on open source maintainers and contributors. And
we scheduled …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week took place in Bilbao, Spain, &lt;a href="https://www.euroscipy.org/2019/"&gt;EuroSciPy 2019&lt;/a&gt;.
This year we introduced the &lt;a href="https://www.euroscipy.org/2019/maintainers.html"&gt;maintainers track&lt;/a&gt;
a room dedicated to discussions among maintainers. The idea is similar to the 
&lt;a href="https://en.wikipedia.org/wiki/Birds_of_a_feather_(computing)"&gt;birds of a feather&lt;/a&gt; or unconference
sessions of other conferences. But focussed on open source maintainers and contributors. And
we scheduled most of the sessions in advanced, to attract the interested people to join the
conference. We also had a maintainers plenary session, in which 26 maintainers of popular
open source scientific projects participated (my guess is that around 50 maintainers attended
the conference).&lt;/p&gt;
&lt;h2&gt;Dataframe summit session&lt;/h2&gt;
&lt;p&gt;One of the sessions was a 2 hours discussion on Python dataframes. 16 people attended it, around
half of them were maintainers of dataframe open source libraries. There were also pandas users
and contributors, maintainers of other projects (PyPy, pytest) and people interested in being involved.
Also the developer of a proprietary dataframe library in Python, who could also add value to the discussion.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/dataframe_summit.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Those were the libraries represented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pandas-dev/pandas"&gt;pandas&lt;/a&gt;&lt;/strong&gt; Flexible and powerful data analysis / manipulation library for Python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/dask/dask"&gt;Dask&lt;/a&gt;&lt;/strong&gt; Parallel computing with task scheduling&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/vaexio/vaex"&gt;Vaex&lt;/a&gt;&lt;/strong&gt; Out-of-Core DataFrames for Python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modin-project/modin"&gt;Modin&lt;/a&gt;&lt;/strong&gt; A dataframe framework that scales the pandas API with &lt;a href="https://github.com/ray-project/ray"&gt;Ray&lt;/a&gt; and &lt;a href="https://github.com/dask/dask"&gt;Dask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/QuantStack/xframe"&gt;xframe&lt;/a&gt;&lt;/strong&gt; DataFrame library in C++&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We started by personal introductions, project introductions, and what people wanted to get out
of the session (many people already proposed topics before the event, and we defined an agenda with those).&lt;/p&gt;
&lt;h3&gt;Document the ecosystem&lt;/h3&gt;
&lt;p&gt;One of the first topics discussed was on how to let users know what is the best dataframe
tool for their job, and how the existing packages are different. The general consensus was
that the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/ecosystem.html"&gt;pandas ecosystem&lt;/a&gt; page
is the best place for it. There are already plans to improve this page (and plans and work in progress to improve
the look and feel of the pandas website and documentation).&lt;/p&gt;
&lt;h3&gt;Apache Arrow&lt;/h3&gt;
&lt;p&gt;Another topic that was discussed early was &lt;strong&gt;&lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;&lt;/strong&gt;. Arrow's mission is to
provide a common memory representation for all dataframe libraries. So, libraries don't need to reinvent the
wheel, and transferring data among packages (e.g. pandas to R) can be done without transformations or even without
copying the memory.&lt;/p&gt;
&lt;p&gt;Vaex is already using Arrow, and pandas has plans in its &lt;a href="https://pandas.pydata.org/pandas-docs/stable/development/roadmap.html"&gt;roadmap&lt;/a&gt;
to move in that direction. People were in general happy with the idea, but there were some concerns
about decisions made in Arrow (mainly contributed by Sylvain, from xframe):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache arrow C++ API and implementation not following common C++ idioms&lt;/li&gt;
&lt;li&gt;Using a monorepo (including all bindings in the same repo as Arrow)&lt;/li&gt;
&lt;li&gt;Not a clear distinction between the specification and implementation (as in for instance project Jupyter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not only related to Arrow, but it was mentioned that would be useful to have
dataframes for streaming data. A library named &lt;a href="https://github.com/finos/perspective"&gt;Perspective&lt;/a&gt;
exists, which implements something similar, and has &lt;a href="https://github.com/timkpaine/perspective-python/"&gt;Python bindings&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Interoperability&lt;/h3&gt;
&lt;p&gt;The next topic was about &lt;strong&gt;interoperability&lt;/strong&gt;. How dataframe libraries can interact among them, and
with the rest of the ecosystem. Examples can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using the same plotting backends from different dataframe libraries&lt;/li&gt;
&lt;li&gt;Passing to &lt;a href="https://scikit-learn.org/stable/index.html"&gt;scikit-learn&lt;/a&gt; pandas-like dataframe objects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There was consensus that defining a standard (and minimal) dataframe API would help. Dataframe libraries
could extend this smaller API and offer users a much bigger APIs (like pandas). But having a subset of
operations and methods would be very useful for third party libraries expecting dataframe objects.&lt;/p&gt;
&lt;p&gt;Devin from Modin is doing research at UC Berkeley on defining this API, and he's already got some
documentation he's happy to share. Modin is already implemented with this design, and while it's
one of the less mature participating projects (in Devin's words), it's user-facing layer could
potentially be reused by other projects reimplementing dataframes with a different backend. Devin
has shared the documentation for this &lt;a href="https://modin.readthedocs.io/en/latest/architecture.html#system-architecture"&gt;design&lt;/a&gt; and the &lt;a href="https://modin.readthedocs.io/en/latest/architecture.html#modin-dataframe-api"&gt;corresponding API&lt;/a&gt; on
the &lt;a href="https://modin.readthedocs.io"&gt;Modin documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It was noted that could be useful to have a common test suite, if a standard dataframe API is defined.
There was agreement that the pandas test suite is not appropriate for other packages.&lt;/p&gt;
&lt;p&gt;NumPy did something similar in &lt;a href="https://numpy.org/neps/nep-0018-array-function-protocol.html"&gt;NEP-18&lt;/a&gt;,
which can be used as reference.&lt;/p&gt;
&lt;h3&gt;Public API improvements&lt;/h3&gt;
&lt;p&gt;At the end of the session, we discussed about possible improvements to the public pandas API.
Since several of the participants reimplemented the pandas API, was a good opportunity to see
places where they found inconsistencies, or where the API was making their lives difficult
when using other approaches.&lt;/p&gt;
&lt;p&gt;Indexing was the part of pandas that other maintainers were less happy about. The way &lt;code&gt;.loc&lt;/code&gt;
behaves was one of the comments. And being forced to have a default index, and not being able
to index by other columns were other comments.&lt;/p&gt;
&lt;h3&gt;Next steps&lt;/h3&gt;
&lt;p&gt;Couple of things were discussed to keep those discussions active, and keep coordinating on
shaping the dataframes of the future.&lt;/p&gt;
&lt;p&gt;The first was to start a workgroup, or a distribution list (or discourse). The &lt;code&gt;pandas-dev&lt;/code&gt;
list wasn't used by the participants (except the pandas maintainers), and it didn't seem
to be the appropriate place.&lt;/p&gt;
&lt;p&gt;Another idea would be to organize another bigger dataframe summit in the future. It was
proposed to be hosted somewhere in the Caribbean (ok, it was me who proposed that, and
everybody else laughed, but here I leave it again). :)&lt;/p&gt;</content><category term="pandas"></category></entry><entry><title>pandas: The two cultures</title><link href="https://datapythonista.github.io/blog/pandas-the-two-cultures.html" rel="alternate"></link><published>2019-07-22T23:26:00+01:00</published><updated>2019-07-22T23:26:00+01:00</updated><author><name>Marc</name></author><id>tag:datapythonista.github.io,2019-07-22:/blog/pandas-the-two-cultures.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="/static/img/blog/two_cultures/leo_breiman.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.stat.berkeley.edu/~breiman/"&gt;Leo Breiman&lt;/a&gt; was a distinguished statistician at
UC Berkeley, known among other things for his major contributions to CART (decision trees),
and ensemble techniques, mainly bootstrap aggregation. Combining both, he was able to define
one of the most popular machine learning models even today (18 years after the publication
of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="/static/img/blog/two_cultures/leo_breiman.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.stat.berkeley.edu/~breiman/"&gt;Leo Breiman&lt;/a&gt; was a distinguished statistician at
UC Berkeley, known among other things for his major contributions to CART (decision trees),
and ensemble techniques, mainly bootstrap aggregation. Combining both, he was able to define
one of the most popular machine learning models even today (18 years after the publication
of the paper), &lt;a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf"&gt;Random forests&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In 2001, Breiman published the paper
&lt;a href="http://www2.math.uu.se/~thulin/mm/breiman.pdf"&gt;Statistical Modeling: The Two Cultures&lt;/a&gt;.
In it, Breiman identified that there were two somehow conflicting cultures in the discipline
of statistical modeling. One that was focusing on modeling (and trying to understand) the
stochastic process generating some random data. While the other followed an algorithmic
approach focused on obtaining results (minimizing the error between the model results and
the data), and considered the stochastic process a black box. Today we would probably call
them &lt;em&gt;statistics&lt;/em&gt; and &lt;em&gt;machine learning&lt;/em&gt;, and the division between them is clear. And in a way,
machine learning is not even considered part of statistics. While this division among the two
fields may or may not be a good thing, identifying in 2001 that both communities existed, were
different and had different needs, surely helped overcome the frustration of both communities at
that time, and sped up their development. One example that illustrate the differences can be
seen on how in the area of neural networks, publishing research papers is mostly driven by 
the obtained results, more than by the theory behind the results. Ali Rahimi gave
&lt;a href="https://www.youtube.com/watch?v=Qi1Yry33TQE"&gt;his view&lt;/a&gt; on this when receiving the test-of-time
award at NeurIPS 2017.&lt;/p&gt;
&lt;p&gt;But this post is not about machine learning, but about &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt;.
And about the two cultures in the pandas community, that I personally don't think are often
well identified, causing frustration to some users, and making more complex taking decisions
regarding the API of the project.&lt;/p&gt;
&lt;h2&gt;Dr Jekyll and Mr Hyde&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/two_cultures/dr_jekyll_mr_hyde.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;To describe the two cultures, let me talk about my own professional experience.
For the last years I've been mainly working as a data scientist. Since the developers of
&lt;a href="https://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt; are doing all the &lt;em&gt;fun&lt;/em&gt; work in machine learning,
and implementing all the complex algorithms for the rest of us, I'll argue that my job (and the
job of many other data scientists, some will probably disagree) is to work on data analysis to
find out what needs to be done, and data engineering to make it work in production.&lt;/p&gt;
&lt;p&gt;What I call &lt;strong&gt;data analysis&lt;/strong&gt; is performed in a &lt;a href="https://jupyter.org/"&gt;Jupyter notebook&lt;/a&gt;,
where I analyze and visualize the data. I found out what is wrong with it, and I quickly
grow the cells of my &lt;code&gt;Untitled23.ipynb&lt;/code&gt; hoping I'll never have to look back at my dirty code.
What I value the most is being able to write code fast, and focus in the problem I'm solving, and
not in the code. To the extend I alias every Python library I import with incomprehensible names
like &lt;code&gt;np&lt;/code&gt;, &lt;code&gt;pd&lt;/code&gt;, &lt;code&gt;plt&lt;/code&gt;,... to make sure I save few microseconds compared to typing the actual
names. And I really appreciate the software making as many decisions as needed to save me from
having to spend the time on being explicit on what I want. Ok, this may be a bit exaggerated,
I don't really let my notebook names be untitled whatever, or use aliases, but I think you get the idea.&lt;/p&gt;
&lt;p&gt;On the other hand, when working in &lt;strong&gt;data engineering&lt;/strong&gt; I use vim, and I write all my code in
Python files in a clear directory structure. Every file and directory are carefully named so I can
easily find them later. Every function is well documented, and the best coding standards are applied.
All my code is version controlled with git, and code reviewed by my colleagues. I write every single
line of code knowing that I will have to revisit it many times, and I optimize for its simplicity and
its clarity.  The thing I'm more adverse to is &lt;em&gt;magic&lt;/em&gt; happening, and any software making decisions
for me. I want to be in control, I want everything in my code to be deterministic, and I want
everything in my code to be explicit. Everything that Tim Peters wrote in
&lt;a href="https://www.python.org/dev/peps/pep-0020/"&gt;PEP-20&lt;/a&gt;, the Zen of Python, applies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beautiful is better than ugly.&lt;/li&gt;
&lt;li&gt;Explicit is better than implicit.&lt;/li&gt;
&lt;li&gt;Simple is better than complex.&lt;/li&gt;
&lt;li&gt;Readability counts.&lt;/li&gt;
&lt;li&gt;Errors should never pass silently.&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;One pandas to rule them all&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/two_cultures/ring.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;What I find the most interesting part about &lt;em&gt;the two cultures&lt;/em&gt; I just described, is that I use
pandas for both. I think pandas is the best tool for both use cases, and I won't admit I'm biased
here, since I'm a pandas maintainer because I use the software, and not the other way round.&lt;/p&gt;
&lt;p&gt;But how is that possible? Both use cases are radically different. Is pandas designed in a way that
is able offer both kind of users the API and features they need? Is that always possible?&lt;/p&gt;
&lt;p&gt;The next of this post will try to find an answer by analyzing some examples.&lt;/p&gt;
&lt;h2&gt;Show me the code&lt;/h2&gt;
&lt;h3&gt;Creating data from a Python dict&lt;/h3&gt;
&lt;p&gt;Let's start with a single example, by manually creating some data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;unicorn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;spider&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;penguin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;spider&lt;/span&gt;     &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;int64&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I think we can agree that pandas is letting us create our data in the simplest possible way. There
could be other ways (and there are other ways that pandas supports), but creating a Series looks to
me as simple as it can be. That's what I want as a data analyst.&lt;/p&gt;
&lt;p&gt;But as a data engineer, there are more things to consider. Imagine that my data, instead of having 3
samples, had 3 million. How much memory is pandas consuming to store in memory my data? And why?
For simplicity, let's consider only the values (and not the name of the animals):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;24&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The values in our Series are consuming 24 bytes. If we see again the representation of our Series,
we can see how the data type (aka dtype) is &lt;code&gt;int64&lt;/code&gt;. Meaning that every value will consume 64
bits (8 bytes). 8 bytes per value, multiplied by 3 values (the number of legs for unicorn, spider
and penguin) totals 24 bytes. But why 64 bits? pandas decided for us that representation, which can
store numbers from around -9e18 to 9e18. But do we really expect animals to have a number of legs
with 18 digits? Or do we expect negative numbers of legs at all? Probably not. We know it, but
pandas doesn't. Because pandas doesn't know anything about our domain, or what is reasonable,
it's deciding for us a conservative representation for our data that won't cause us problems
(as opposed as one that saves some memory).&lt;/p&gt;
&lt;p&gt;This is working well for us as data analysts, but not as data engineers writing production code.
In this case, the Series constructor has a parameter &lt;code&gt;dtype&lt;/code&gt; that we can use to tell pandas to not
decide for us how to internally represent the data, but to tell it explicitly. This is the result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;unicorn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;spider&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;penguin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;                          &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;uint8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;    &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;spider&lt;/span&gt;     &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;uint8&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this example, pandas provides a reasonable API for both kind of users. It doesn't force us to
specify the data type when we don't care. But we're able to when we do care. Whether we want to
optimize for our system resources (mainly memory) or our own time is up to us.&lt;/p&gt;
&lt;h3&gt;How many legs do unicorns have?&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/two_cultures/unicorn.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;An important question we face is, how many legs do unicorns have? In the previous example, we specified
they have 4, but do unicorns really have 4 legs? Did anybody have ever seen a unicorn? Let's try to be
prudent and say that we don't know how many legs they have. By convention, when we have an unknown
or missing value, we represent it as &lt;code&gt;NaN&lt;/code&gt; (Not a Number). Every number in a computer is represented
using binary numbers (e.g. &lt;code&gt;01001011&lt;/code&gt;). &lt;code&gt;NaN&lt;/code&gt; is represented internally as one specific sequence of
bits, reserved to have the meaning of &lt;code&gt;NaN&lt;/code&gt;. There is a convention that &lt;em&gt;translates&lt;/em&gt; how every binary
sequence corresponds to the number they represent. And this &lt;em&gt;translation&lt;/em&gt; has some exceptions, including
one value that represents the floating point number &lt;code&gt;NaN&lt;/code&gt;. If that sounds too complex, think that in
binary, &lt;code&gt;0000&lt;/code&gt; can represent the number 0, &lt;code&gt;0001&lt;/code&gt; the 1, &lt;code&gt;0010&lt;/code&gt;: 2, &lt;code&gt;0011&lt;/code&gt;: 3... and &lt;code&gt;1111&lt;/code&gt;: 15.
And what microprocessors manufacturers decided is something like letting represent only from 0 to 14
(instead of from 0 to 15, that we could encode with 4 bits), and reserve the &lt;code&gt;1111&lt;/code&gt; to mean &lt;code&gt;NaN&lt;/code&gt;.
Things are in reality more complex, since &lt;code&gt;NaN&lt;/code&gt; representations only exists for floating points numbers
(aka float), which are decimals. But that explanation should give an intuition.&lt;/p&gt;
&lt;p&gt;So, back to the example, if we want to represent that we don't know how many legs unicorns have, we
can simply do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;unicorn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;NaN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;    &lt;span class="n"&gt;NaN&lt;/span&gt;
&lt;span class="n"&gt;spider&lt;/span&gt;     &lt;span class="mf"&gt;8.0&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;    &lt;span class="mf"&gt;2.0&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;float64&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Many things happened here. We can see, how besides the expected change of having &lt;code&gt;NaN&lt;/code&gt; unicorn legs,
now we are back to consuming 64 bits. And not only that, but also the rest of values in the column now are
decimal (float) values. As I just explained, and can also be seen in the example on how &lt;code&gt;NaN&lt;/code&gt; is created,
&lt;code&gt;NaN&lt;/code&gt; is a float value. Modern computers don't have an integer representation for &lt;code&gt;NaN&lt;/code&gt;, so for pandas
to do what we asked it to do, converting the column to float was the &lt;em&gt;only&lt;/em&gt; option (not really the only,
but let's pretend for a second).&lt;/p&gt;
&lt;p&gt;It feels a bit weird to see in the Series representation that a penguin has 2.0 legs. It's conceptually
wrong, and also misleading making us believe that animals can have a decimal number of legs. There are
also technical implications too, we are consuming 4 times more memory now. And also operations among
integers don't take the same time as operations among floats at the CPU level (note that while floats
are a more complex representation, modern CPU's are highly optimized for them, and operations can even be
faster for floats than for integers).&lt;/p&gt;
&lt;p&gt;But there is something else, see this example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;
&lt;span class="bp"&gt;False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Floating point numbers are approximations. They are mapping an infinite set of numbers (let's say all
real numbers) to the finite set of possible representations with 64 bits (&lt;code&gt;2 ** 64&lt;/code&gt;). In many
cases using this approximate values won't make a difference (the height of a person keeps being the
same if we change the 20th decimal). But, if for example a column contains an integer id that we use
to join two data sets, converting it to floating point can mean data loss or bugs. Since floating points
are just approximations, we may try to join by &lt;code&gt;20.0000000001 == 19.9999999999&lt;/code&gt;, which won't match.
So, converting an integer column to its floating point representation can be dangerous, and probably
more for the data engineering use cases described before.&lt;/p&gt;
&lt;p&gt;In pandas 0.24 we introduced a new data type to mix integer values with missing values. This is done
by instead of using the float &lt;code&gt;NaN&lt;/code&gt; to represent the missing values, we internally keep a separate
Boolean array that identifies where the missing values are. This adds an extra layer of complexity
inside pandas, but avoids problems like the one just described. By default, pandas still uses the
original types, but we can write the previous code as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;unicorn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;spider&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;penguin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;                          &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;UInt8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;unicorn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;NaN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;    &lt;span class="n"&gt;NaN&lt;/span&gt;
&lt;span class="n"&gt;spider&lt;/span&gt;       &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;      &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;UInt8&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that &lt;code&gt;UInt8&lt;/code&gt; represents the pandas type with the mask, and &lt;code&gt;uint8&lt;/code&gt; (lowercase) represents the
original type based on numpy. Also note that the new type may not be as stable as the old, and may not
implement all the operations.&lt;/p&gt;
&lt;p&gt;While the new data type fixes this specific problem, the fact that pandas silently casts a data type
when needed is very convenient for the use cases of data analysts, but in my opinion does a poor job
to the interests of precision and reliability of data engineers. And while the &lt;code&gt;.loc[]&lt;/code&gt; syntax is very
convenient, doesn't allow us to solve the problem with a simple parameter. A new
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html"&gt;pandas option&lt;/a&gt; could be an option
to control whether we want pandas to automatically cast columns when needed, or raise an exception instead.
But as far as I know, there has not been discussion about it.&lt;/p&gt;
&lt;h2&gt;The most popular pandas function&lt;/h2&gt;
&lt;p&gt;CSV is in general a poor format to store data. It has a clear advantage, that is being able
to open CSV files in a text editor. Other than that, I think all are disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inefficient storage (space that file uses in disk)&lt;/li&gt;
&lt;li&gt;Inefficient I/O (because the volume of data, and also the required casting)&lt;/li&gt;
&lt;li&gt;Lack of types (everything is a string in a CSV, so original types are lost)&lt;/li&gt;
&lt;li&gt;Lack of a standard (different quoting, delimiters,...)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite of those, CSV happens to be one of the most popular formats out there, being the
page &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"&gt;pandas.read_csv&lt;/a&gt;
the one with most visits in the pandas documentation.&lt;/p&gt;
&lt;p&gt;To manage all the trickiness of the format, &lt;code&gt;pandas.read_csv&lt;/code&gt; provides as much as 50
arguments, to customize for your file format, and for your needs.
&lt;a href="https://github.com/InvestmentSystems/static-frame"&gt;StaticFrame&lt;/a&gt; a project (somehow)
aiming to compete with pandas, contains the next sentence in its README file:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Pandas CSV reader far out-performs the NumPy-based reader in StaticFrame: thus, for now, using Frame.from_pandas(pd.read_csv(fp)) is recommended for loading CSV files.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This gives an idea of all the complexity in the CSV parser, not only in terms of the parameters,
but also in terms of how optimized it is for performance.&lt;/p&gt;
&lt;p&gt;Despite being one of the most powerful and optimized CSV parsers out there,
&lt;a href="http://twitter.com/dontusethiscode"&gt;James Powell&lt;/a&gt; gave a
&lt;a href="https://www.youtube.com/watch?v=QkQ5HHEu1b4&amp;amp;t=1554"&gt;lightning talk at PyData London 2019&lt;/a&gt;
on how the parser could be easily improved in several ways for a use case he's got.&lt;/p&gt;
&lt;p&gt;Those include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume string columns are properly encoded and load them directly into memory&lt;/li&gt;
&lt;li&gt;Optimize date casting by assuming a specific format, and a limited set of values&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, no matter the great job done in implementing pandas, the software is
being unable to fully satisfy all user cases. &lt;code&gt;pandas.read_csv&lt;/code&gt; does again a
good job at making life easy to data analysts (as defined at the beginning of this
post). And it also does an impressive job at adding parameters to empower users that
know what they are doing and have production-ready code need (data engineers). But
even with an insane number of parameters like 50, looks like loading a CSV file into
memory may be too complex for a single generic function.&lt;/p&gt;
&lt;p&gt;What is the solution here? Personally, I think that having &lt;em&gt;one pandas to rule them all&lt;/em&gt;
is still possible and the best option. But not a &lt;code&gt;pandas.read_csv&lt;/code&gt; to rule them all.
My view is that pandas shouldn't include I/O modules that are able to load data from
every possible format, and in every possible way. That's just impossible.
But pandas could do a better job at allowing and encouraging an ecosystem of I/O
pandas plugins. I proposed in &lt;a href="https://github.com/pandas-dev/pandas/issues/26804"&gt;this issue&lt;/a&gt;
a first refactoring that would make this possible. It is still under discussion,
since the proposed changes are big.  I'll write in a different article more details about this proposal.&lt;/p&gt;
&lt;h2&gt;Lazy pandas&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/two_cultures/lazy_pandas.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;To conclude this article, I will talk about what in my opinion is one of the biggest
differences between the needs of data analysts using pandas in a Jupyter notebook,
compared to data engineers using it to write production pipelines.&lt;/p&gt;
&lt;p&gt;See this example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;unicorn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;spider&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;penguin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="mf"&gt;4.0&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;
         &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;spider&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;legs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;
         &lt;span class="n"&gt;legs&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;     &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;spider&lt;/span&gt;      &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;kind&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;legs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;biped&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;                                              &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;quadruped&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;                                              &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;octoped&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;
         &lt;span class="n"&gt;legs&lt;/span&gt;       &lt;span class="n"&gt;kind&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;     &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="n"&gt;quadruped&lt;/span&gt;
&lt;span class="n"&gt;spider&lt;/span&gt;      &lt;span class="mi"&gt;8&lt;/span&gt;    &lt;span class="n"&gt;octoped&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;      &lt;span class="n"&gt;biped&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legs&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
         &lt;span class="n"&gt;legs&lt;/span&gt;       &lt;span class="n"&gt;kind&lt;/span&gt;
&lt;span class="n"&gt;unicorn&lt;/span&gt;     &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="n"&gt;quadruped&lt;/span&gt;
&lt;span class="n"&gt;penguin&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;      &lt;span class="n"&gt;biped&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_legs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_parquet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;num_legs.parquet&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And compare it with this other code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;unicorn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;spider&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;penguin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;legs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;legs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;biped&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;                                                    &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;quadruped&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;                                                    &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;octoped&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;legs &amp;lt;= 4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_parquet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;num_legs.parquet&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Before you are tempted to think on which one is better, let's discuss which
problem solves each of them.&lt;/p&gt;
&lt;p&gt;The first version is part of an iterative process where at every step we need
to visualize how our data looks like. We also may need not only to visualize the
data, but &lt;em&gt;understand&lt;/em&gt; or verify it, for example by checking which is the median
of one column. It is likely that at the end of writing that code, we don't care
about it anymore, since we already verified what was in the data, and extracted
the insights we care about.&lt;/p&gt;
&lt;p&gt;In the second case, while doing almost the same, the code is written to be read
and to be maintained. If there is a bug in the code, it should be easy to
understand what it does, and fix it. The goal is not to discover anything
while writing the code. But just to add a functionality to a system, and to be
able to run it in a reliable and performant way.&lt;/p&gt;
&lt;p&gt;For more information about the style in the second approach, you can check
the must-read &lt;a href="https://tomaugspurger.github.io/method-chaining"&gt;Method Chaining&lt;/a&gt;
by the pandas maintainer &lt;a href="https://tomaugspurger.github.io/pages/about.html"&gt;Tom Augspurger&lt;/a&gt;.
Also, I discussed about method chaining in my talk
&lt;a href="https://www.youtube.com/watch?v=hK6o_TDXXN8"&gt;Towards pandas 1.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Back to the example, pandas let us write code in a way that suits both
data analysts and data engineers. But there is something else that is worth
considering. In the first version, the operations must be executed one at
a time, since they are independent. But in the example using method chaining,
there is no need to execute anything until &lt;code&gt;to_parquet&lt;/code&gt; is run. The reason
is that the result is not made available to the user or anywhere else.&lt;/p&gt;
&lt;p&gt;This may sound irrelevant at first, since we are going to execute it anyway.
But being able to postpone the actual execution until a later stage, can
be extremely useful in some situations. In the example, if pandas postpones
the execution until it knows all what the user wants to do with all the data,
it could optimize the execution. For example, if the row of the spider is
going to be discarded, why load it to memory and why compute which is its
kind? Some memory and some computation power and time can be saved. In this
toy example it doesn't make a difference, but imagine you want to operate
with 1Tb of data in a file, apply some transformations,
and save the result in another file in disk. With the &lt;em&gt;data analyst approach&lt;/em&gt;
this is not feasible when running the code in a normal laptop. And while
pandas is not able to work in an out-of-core way, or optimize the execution
even when using method chaining, that could be implemented.&lt;/p&gt;
&lt;p&gt;There are related tools where this lazy execution approach already
exists, mainly &lt;a href="https://dask.org/"&gt;Dask&lt;/a&gt;. Dask implements a
pandas-like API, where operations are evaluated in a lazy way, and the
final task graph is not only optimized, but distributed over a cluster.
&lt;a href="https://github.com/vaexio/vaex"&gt;Vaex&lt;/a&gt; is another example of pandas-like
API implemented with lazy evaluation.
&lt;a href="https://youtu.be/2Tt0i823-ec"&gt;This talk&lt;/a&gt; has a demo showing how Vaex
uses lazy evaluation to deal with data sets with more than one billion
rows.&lt;/p&gt;
&lt;p&gt;Lazy evaluation may be out of scope for pandas, and there are many things
that should be changed even before being considered. But in my opinion is
another example on the different needs of the different pandas users.&lt;/p&gt;
&lt;p&gt;I guess a dual pandas would be possible, and for the user, may be a
simple pandas option &lt;code&gt;pandas.options.lazy_execution = True&lt;/code&gt; would be enough.
Together with few methods to allow users to trigger the execution of a task
graph (e.g. a &lt;code&gt;.collect()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;There are also other approaches that could be considered. With the recent
addition of &lt;a href="https://pandas.pydata.org/pandas-docs/stable/development/extending.html#extension-types"&gt;pandas extension arrays&lt;/a&gt;,
custom data types can be implemented. And having types for memory
maps, or calculated columns could be an option that could allow
some sort of laziness. In the example, we could have a normal
DataFrame, that could have a kind column that does not actually save
the strings &lt;code&gt;biped&lt;/code&gt;, &lt;code&gt;quadruped&lt;/code&gt;,... but instead stores the
function applied, and to which column. The actual lookup could then
happen after the data is filtered.&lt;/p&gt;
&lt;p&gt;Whatever could be the approach, it would require major changes to
pandas internals, and it's not something that could be implemented
easily. Custom data types can be implemented, but currently some
operations will convert the data to numpy arrays, and would not
allow having a proper lazy data type.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I think the number of pandas users, and the different kinds of work
that are being done are evidence of many good design decisions and
implementation. But conflicting interests among groups of users do
exist. In some cases is doable to find a good solution for most
use cases. In others is not obvious and serving better our users would
require a huge amount of work.&lt;/p&gt;
&lt;p&gt;Personally, I think a more modular pandas architecture would make it
easier to adjust to every kind of user. By having more than one version
of &lt;code&gt;pandas.read_csv&lt;/code&gt; different users could implement solutions that
better suit their needs. Same could apply to other areas.&lt;/p&gt;
&lt;p&gt;But probably the most important challenge to get those implemented is
not what is the technical solution, but it's in how pandas is developed.
The project is mostly developed by volunteers, including the maintainers
(the people who review the contributions, discuss in the issues that
users open...). Our roadmap is not determined by the needs
of your company or your industry. In my personal case, my roadmap
is determined by my personal interests on what I want to work on,
and on the kind of things I need or I want to see in pandas myself.
If your company would be more productive with certain pandas features or
developments, you should consider hiring someone to improve pandas
based in your interests. You can contact &lt;a href="https://numfocus.org/"&gt;NumFOCUS&lt;/a&gt;
who manages the pandas funding, can assist with any question, and
is in direct contact with the pandas maintainers. Besides hiring someone
in your own team, you could also provide funds to develop pandas that
are managed by NumFOCUS. Also feel free to &lt;a href="mailto:garcia.marc@gmail.com"&gt;contact me&lt;/a&gt;
directly if you want more advice, and are interested in this.&lt;/p&gt;</content><category term="pandas"></category></entry><entry><title>#pandasSprint write-up</title><link href="https://datapythonista.github.io/blog/pandassprint-write-up.html" rel="alternate"></link><published>2018-03-22T01:57:00+00:00</published><updated>2018-03-22T01:57:00+00:00</updated><author><name>Marc</name></author><id>tag:datapythonista.github.io,2018-03-22:/blog/pandassprint-write-up.html</id><summary type="html">&lt;p&gt;The past 10th of March took place &lt;a href="https://python-sprints.github.io/pandas/"&gt;#pandasSprint&lt;/a&gt;.
To the best of my knowledge, an unprecedented kind of event, where around 500 people worked
together in improving the documentation of the popular pandas library.&lt;/p&gt;
&lt;p&gt;As one of the people involved in the organization of the event, I wanted to write …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The past 10th of March took place &lt;a href="https://python-sprints.github.io/pandas/"&gt;#pandasSprint&lt;/a&gt;.
To the best of my knowledge, an unprecedented kind of event, where around 500 people worked
together in improving the documentation of the popular pandas library.&lt;/p&gt;
&lt;p&gt;As one of the people involved in the organization of the event, I wanted to write about why
I think this event was much more than the contributions sent, and the fun day we had. And
also provide information on how it was planned, to help future organizers.&lt;/p&gt;
&lt;h2&gt;Some historical context&lt;/h2&gt;
&lt;p&gt;To explain where the idea of the #pandasSprint came from, I need to go back in time more
than 15 years. Those were the times where open source was named free software, people queued
to see &lt;a href="https://en.wikipedia.org/wiki/Richard_Stallman"&gt;Richard Stallman&lt;/a&gt; talks, and
companies like SCO and Microsoft were in the dark side of proprietary software. Free
software was more about freedom than about software, and the free software community was
working hard and united to build the software that could challenge the status quo.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://trisquel.info/files/richard%20stallman.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Now we’re in 2018, and things changed a lot. SCO doesn’t exist anymore, and Microsoft is one
of the companies supporting more open source. Employing more Python core developers than any
other company, sponsoring major events like PyCon or EuroPython, and funding non-profits
like &lt;a href="https://www.numfocus.org/"&gt;NumFOCUS&lt;/a&gt;,
&lt;a href="https://www.python.org/psf/"&gt;The Python Software Foundation&lt;/a&gt; and even
&lt;a href="http://www.linuxfoundation.org/"&gt;The Linux Foundation&lt;/a&gt;. Python is growing in popularity, and
nobody questions the advantages of open source software.&lt;/p&gt;
&lt;p&gt;But what happened to all the free software hackers who untiringly were making their projects
be to the highest standards? Of course there are still many people there, but my perception
is that the growth in popularity of open source projects didn’t translate linearly to a
growth in the number of contributors. And I think pandas is one of the clearest examples.&lt;/p&gt;
&lt;p&gt;For the last years, pandas has been becoming a de-facto standard in data analytics and data
science. Recently, Stack Overflow published that
&lt;a href="https://stackoverflow.blog/2017/09/14/python-growing-quickly/"&gt;almost 1% of their traffic from developed countries is caused by pandas&lt;/a&gt;.
The book Python for data analysis by pandas creator
&lt;a href="https://twitter.com/wesmckinn/status/974303935530876928"&gt;sold more more 250,000 copies&lt;/a&gt;,
and the pandas website has around
&lt;a href="https://twitter.com/jorisvdbossche/status/974322924034449408"&gt;400,000 activeusers per month&lt;/a&gt;.
It’s difficult to know how many pandas users exist, but some
&lt;a href="https://twitter.com/teoliphant/status/974056911627866113"&gt;informed opinions talk about 5 million&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://stackoverflow.blog/wp-content/uploads/2017/09/related_tags_over_time-1-1200x1200.png"&gt;&lt;/p&gt;
&lt;p&gt;What about the contributors? In a quick look at
&lt;a href="https://github.com/pandas-dev/pandas/graphs/contributors"&gt;GitHub&lt;/a&gt;, I counted 12 developers
that have been active in the last year, and that contributed more than 20 commits to the project.
This leaves a ratio of 1 significant contributor for more than 400,000 users. Not long before
the #pandasSprint the project achieved 1,000 contributors. Meaning that 1 in each 5,000 ever made
a contribution.&lt;/p&gt;
&lt;p&gt;You can find these small or big depending on your expectations. And it’s difficult to compare
without numbers about Python projects 10 years ago. But my feeling is that we transitioned from a
free software community of developers actively participating in the projects, to a community of
mainly users, who in many cases see free software as
&lt;a href="https://en.wikipedia.org/wiki/Free_as_in_Freedom"&gt;free beer&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;How to become part of the open source community&lt;/h2&gt;
&lt;p&gt;I don’t know why people become part of the open source community, in terms of participating
actively on it. But I know how I did. It’s a beautiful and sad story that I want to share.&lt;/p&gt;
&lt;p&gt;Around 12 years ago, I was quite new to Python, but really liking the language compared to
what I used before. Most of what I was doing was web based, so I quickly discovered Django,
and felt in love. What in PHP (the de-facto standard at that time) took one week or more to
implement, in Django was done in minutes, and with much higher quality. Django was simply
amazing, the web framework for perfectionists with deadlines. But in some areas not as
mature as it is now. And I’m talking mainly about localization. The system to translate
static text was amazing, but you couldn't make calendars start in Monday, or use the comma
as a decimal separator. That was a big problem for me, as my users in Spain wouldn't be
happy using the US localization. The good news was that it was open source, so I started to
take a look on what could be done.&lt;/p&gt;
&lt;p&gt;When I submitted my first bug reports and patches to Django, I found the best mentor a
newcomer to open source can find, Malcolm Treddinick. He was the core developer more
involved in the localization part of Django. Malcolm helped me in every step, and I learned
a lot from him about Python, Django, subversion... But I also learned from him (and also
from others in the community) about kindness and collaboration. It was a really welcoming
community, and honestly, at the beginning I found it quite surprising the amount of time
people was happy to spend helping and giving support to someone who didn’t have so much to
contribute. After some time, I managed to be more experienced, and I was able to contribute
back, taking care of the Catalan and Spanish translations for some years, and doing a major
&lt;a href="https://datapythonista.blogspot.co.uk/2009/12/new-localization-system-already-in.html"&gt;refactoring of Django's localization system&lt;/a&gt;,
as part of a &lt;a href="https://summerofcode.withgoogle.com/"&gt;Google summer of code&lt;/a&gt;. But who could
know that beforehand.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://puzzling.org/wp-content/uploads/2013/03/2834869959_85974cbd42_b-248x300.jpg"&gt;&lt;/p&gt;
&lt;p&gt;I was in shock when in 2013
&lt;a href="https://www.djangoproject.com/weblog/2013/mar/19/goodbye-malcolm/"&gt;Malcolm passed away&lt;/a&gt;.
Besides being a tragedy for him and his close ones, it was also for many of us, who barely
met him in person, but considered him a friend. The Django Software Foundation created the
&lt;a href="https://www.djangoproject.com/foundation/prizes/"&gt;Malcolm Tredinnick Memorial Prize&lt;/a&gt; in
his honor. The prize is awarded, quoting the DSF page “to the person who best exemplifies
the spirit of Malcolm’s work - someone who welcomes, supports and nurtures newcomers;
freely gives feedback and assistance to others, and helps to grow the community”.&lt;/p&gt;
&lt;p&gt;Malcolm was unique, but the open source community is the amazing community it is, because
there are so many amazing people who exemplifies the spirit of Malcolm every day.&lt;/p&gt;
&lt;h2&gt;London Python Sprints&lt;/h2&gt;
&lt;p&gt;So, with such an amazing community (and I experienced it enough to be sure about it), what
is it preventing more people to get involved? I would say most people thinks that
technically speaking, they are not good enough for the projects. That you need the mind of
&lt;a href="https://en.wikipedia.org/wiki/Alan_Turing"&gt;Alan Turing&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/Dennis_Ritchie"&gt;Dennis Ritchie&lt;/a&gt; or
&lt;a href="https://en.wikipedia.org/wiki/Linus_Torvalds"&gt;Linus Torvalds&lt;/a&gt; to make a contribution. I
strongly disagree. Even the less technical people can participate in many things such as
translations, writing documentation, ticket triaging… There are also many great projects in
their early stages were contributing code is much easier than contributing to the more
complex and intimidating ones.&lt;/p&gt;
&lt;p&gt;Then, what’s the problem? Personally, I think the only problem is getting started. The first
time, it’s difficult to find a task to get started. It’s difficult to understand the
&lt;a href="https://docs.google.com/presentation/d/1rOSYXZPyMe9KXnbVK_xbJzw_-ijxd6bIxndmvPU6L2o/edit?usp=sharing"&gt;logistics of sending a pull request&lt;/a&gt;.
It’s difficult to know beforehand whether project maintainers will welcome our small
contributions. And it may be difficult to even know that we need a task to work in, that we
need to send a pull request, or that there is a community out there working on every project.
But these are just difficult until someone is able to help you get started.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://secure.meetupstatic.com/photos/event/5/e/a/f/highres_465084239.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;With this idea in mind, &lt;a href="https://www.meetup.com/Python-Sprints/"&gt;London Python Sprints&lt;/a&gt; was
born. A place where open source contributors could mentor newcomers in their first steps. And
personally, I think it’s very successful. Not only we managed to send around 50 pull requests
to different projects in 2017, but people who did the first pull request with us, are now the
mentors helping others get started.&lt;/p&gt;
&lt;h2&gt;#pandasSprint: the idea&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="https://secure.meetupstatic.com/photos/event/6/2/2/1/highres_468505121.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;While the experience in London was great, it was very low scale. And we could do much better.
All it takes for many people to love becoming a contributor, is to have some guidance in these
first steps. We already had the experience from several months of sprints in London, and with
some preparation we could help other user groups do the same.&lt;/p&gt;
&lt;p&gt;Why pandas? There are plenty of great projects to contribute to. But for pandas... Everybody
loves pandas, it’s very popular. It’s a welcoming project in the spirit of Malcolm. Improving
the documentation would be something very useful. And it’s one of the projects I’m more
familiar with&lt;/p&gt;
&lt;p&gt;But it’s probably clear that the goal wasn’t that much about the specific project or
contributions. But about letting people get into the open source world in the way many of us
love it. Becoming part of it, and not just being a user of some software we don’t need to pay
for.&lt;/p&gt;
&lt;h2&gt;#pandasSprint: the implementation&lt;/h2&gt;
&lt;p&gt;So, we wanted to have a huge open source party, but of course that required a huge amount of
work.&lt;/p&gt;
&lt;p&gt;The first thing was to make sure the pandas core developers were happy with it. It was going
to be a lot of work from their side, and they know much more about pandas than anyone else,
and could tell whether it was a good idea, or provide useful feedback. An email to
&lt;a href="https://twitter.com/jreback"&gt;Jeff Reback&lt;/a&gt; was enough to start. He loved the idea, even if I
think he didn’t believed at that time it was going to be something as big as it finally was. :)&lt;/p&gt;
&lt;h3&gt;Dividing the work&lt;/h3&gt;
&lt;p&gt;The next thing was to make sure everybody had something to work on the day of the sprint.
Working on the documentation made it possible. There are around
&lt;a href="https://docs.google.com/spreadsheets/d/10EpQFkVDqiIFLLVGtIWzCMRACz20yWuta3_DU0qV6-E/edit?usp=sharing"&gt;1,200 API pages&lt;/a&gt;
in the pandas documentation. Writing a script to get the list was easy. We could even gather
some information on the state of the documentation (which pages had examples, which methods
had mistakes in their documented parameters...).&lt;/p&gt;
&lt;p&gt;The trickiest part was the system to share docstrings in pandas. There are many functions and
methods in pandas, that are similar enough to have a shared template for the documentation,
customized with few variables specific to each page. The original idea was to use Python
introspection system to find the exact ones sharing a template, so we could avoid duplicates.
That was more complex than it originally seemed, and we finally delegated the task of finding
out to each user group. &lt;/span&gt;&lt;span style="font-weight: normal;"&gt;To help with that, we
divided the pages in groups by topics, and assigned whole groups to each sprint chapter.
Sharing of docstrings was more likely to happen inside these groups. For example, all the
functions in Series.str where in a group. Functions like
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.lower.html"&gt;lower()&lt;/a&gt;,
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.upper.html"&gt;upper()&lt;/a&gt;,
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.capitalize.html"&gt;capitalize()&lt;/a&gt;
use the same template, so it should be somehow easy to detect it in the chapter working on that group.&lt;/p&gt;
&lt;h3&gt;Documentation&lt;/h3&gt;
&lt;p&gt;Then, after being able to provide each participant a task, we had to make sure everybody knew
what to do. For it, there were two main things. First, having documentation explaining all the
steps. And second having mentors in every city.&lt;/p&gt;
&lt;p&gt;For the documentation, we had 3 main documents:
- &lt;a href="https://python-sprints.github.io/pandas/guide/pandas_setup.html"&gt;Set up instructions&lt;/a&gt;
  (installing requirements, cloning the repository, compiling C extensions...)
- &lt;a href="http://pandas-docs.github.io/pandas-docs-travis/contributing_docstring.html"&gt;Guide&lt;/a&gt;
  on how to write a docstring for pandas
- &lt;a href="https://python-sprints.github.io/pandas/guide/pandas_pr.html"&gt;Instructions&lt;/a&gt;
  on how to validate the changes, and submit them&lt;/p&gt;
&lt;p&gt;The most complex part was defining how a “perfect” docstring had to look like. Following some
standards would be very useful for pandas users. All the pages would be implemented in the best
possible way we could think of. And users would be able to get used to one format, and find
information faster.&lt;/p&gt;
&lt;p&gt;We started with a draft of a guide in the form of
&lt;a href="https://github.com/pandas-dev/pandas/pull/19704/files"&gt;pull request&lt;/a&gt;, so everybody could
review and add comments. And then it was a bit of discussion on the topics with disagreements
or unclear. I think the result was great. But of course we couldn’t anticipate all the cases.&lt;/p&gt;
&lt;p&gt;We also had to write &lt;a href="https://github.com/pandas-dev/pandas/pull/20016/files"&gt;documentation&lt;/a&gt;
about shared docstrings, and what was the preferred way to implement it.
&lt;a href="https://twitter.com/TomAugspurger"&gt;Tom Augspurger&lt;/a&gt; took care of it.&lt;/p&gt;
&lt;h3&gt;Mentoring&lt;/h3&gt;
&lt;p&gt;A key thing was to make sure in every location we had people who could mentor participants.
We created a &lt;a href="https://gitter.im/py-sprints/pandas-doc"&gt;gitter channel&lt;/a&gt; for the event, but it
would be difficult to remotely help in more than specific things. Everybody was in their own
local sprint, and we also had different time zones, so availability during the sprint would
be limited.&lt;/p&gt;
&lt;p&gt;So, what we did was to ask
&lt;a href="https://docs.google.com/spreadsheets/d/138095mUxOTOCCXmvQGz7YOh-0yWLoTH_8_IlrAI5w2c/edit?usp=sharing"&gt;somebody from each chapter to work on a taskbefore the sprint&lt;/a&gt;.
In most cases that was the same organizers. I don't know if that is true, but I had the
feeling that some organizers were underestimating how complex improving a single API
documentation page is. And how difficult is to help a large group of people who is doing
their first open source contribution can be. Letting them prepare before hand should be
useful in different ways: Organizers would be better prepared, and have a better sprint,
without so much stress and uncertainty. They should be able to help participants better.
The "mini" sprint of the organizers would be a proof of concept that would let us
anticipate problems in the documentation, the procedure...&lt;/p&gt;
&lt;p&gt;Not all the organizers found the time to prepare, as we were ready to start this stage
less than a week before the global sprint date. But I think it was very useful for the
ones who could prepare for the sprint.&lt;/p&gt;
&lt;h3&gt;Tools&lt;/h3&gt;
&lt;p&gt;One of the areas we worked on preparing the sprint, was in having better tools.
&lt;a href="https://twitter.com/jorisvdbossche"&gt;Joris Van den Bossche&lt;/a&gt;, besides being key in all
the parts of the sprint, did an amazing job on this part. We implemented a way to
&lt;a href="https://github.com/pandas-dev/pandas/pull/19840/files"&gt;build a single document in Sphinx&lt;/a&gt;,
and a &lt;a href="https://github.com/pandas-dev/pandas/blob/master/scripts/validate_docstrings.py"&gt;script to validated formatting errors in docstrings&lt;/a&gt;.
We also set up a &lt;a href="https://github.com/pandas-dev/pandas/pull/20015/files"&gt;sphinx plugin to easily include plots in the documentation&lt;/a&gt;,
which &lt;a href="http://pandas-docs.github.io/pandas-docs-travis/generated/pandas.DataFrame.plot.kde.html"&gt;made some pages look really great&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Last minute, we also build a &lt;a href="https://python-sprints.github.io/pandas/dashboard.html"&gt;dashboard&lt;/a&gt;
with a list of checkpoints that the users could follow during the day, so it was
clearer to know what to do, and it should help them make better contributions.&lt;/p&gt;
&lt;h3&gt;Promotion&lt;/h3&gt;
&lt;p&gt;Promoting the event, and finding the people willing to participate was done in
different ways: The first one was to direct message the organizers of different
communities. Among all the great things of the Python community, is how well
organized it is. In a &lt;a href="https://www.meetup.com/pro/pydata"&gt;single page&lt;/a&gt; there are
the links to the almost 100 PyData meetups all around the world. In the Python
website there is a &lt;a href="https://wiki.python.org/moin/LocalUserGroups"&gt;wiki&lt;/a&gt; with
tens of Python user groups. Not everybody we contacted was interested, or even
answered, but most of the groups were really happy with the idea.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.python.org/psf/"&gt;The Python Software Foundation&lt;/a&gt;, &lt;a href="https://www.numfocus.org/"&gt;NumFOCUS&lt;/a&gt;
were also key in spreading the word about the event.&lt;/p&gt;
&lt;p&gt;As the sprint was to work on the documentation, we also contacted
&lt;a href="http://www.writethedocs.org/"&gt;Write the docs&lt;/a&gt;, a global community focused on
writing technical documentation. Some of their members joined the sprint too.&lt;/p&gt;
&lt;h2&gt;The sprint&lt;/h2&gt;
&lt;p&gt;For the day of the sprint, we've got a last minute surprise. I really think
what every participant of the sprint was going to do, was something really
great. Even if in a way it felt more like a Saturday with friends. And I
think it was worth that people knew how important is to contribute to the
open source projects that power from the scientific research to the
financial markets, or the data science infrastructure of so many companies
in the world. So, just few hours after the sprint we spoke with
&lt;a href="https://twitter.com/wesmckinn"&gt;Wes McKinney&lt;/a&gt;, creator of pandas,
&lt;a href="https://twitter.com/NaomiCeder"&gt;Naomi Ceder&lt;/a&gt;, chair of the Python Software
Foundation, and Leah Silen, executive director at
&lt;a href="https://twitter.com/NumFOCUS"&gt;NumFOCUS&lt;/a&gt;, to see if they could record a
short message to the participants. Even with the very short notice, all them
sent really great messages that we could show the participants at the
beginning of the sprints.&lt;/p&gt;
&lt;iframe allowfullscreen="" class="YOUTUBE-iframe-video"
        data-thumbnail-src="https://i.ytimg.com/vi/YnFKV2oxs8Q/0.jpg"
        frameborder="0" height="266"
        src="https://www.youtube.com/embed/YnFKV2oxs8Q?feature=player_embedded" width="320"&gt;&lt;/iframe&gt;

&lt;p&gt;It's difficult to know what happened in the sprint at a global scale. I
think in London we've got a great time, with nice atmosphere and a luxury
location provided by our sponsor &lt;a href="https://twitter.com/TechAtBloomberg"&gt;Bloomberg&lt;/a&gt;.
I think for most of us the sprint seemed too short. Even if I think it was
a typical British pub follow up to the sprint, that I couldn't join.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://secure.meetupstatic.com/photos/event/a/5/1/5/highres_469122261.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;In other locations, for what I know the experience was also good. It's worth
taking a look at the &lt;a href="https://twitter.com/hashtag/pandasSprint"&gt;twitter feed of the sprint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://2.bp.blogspot.com/-Cyqt0qDvzfU/WrL7uGMLE-I/AAAAAAAAy3o/wiQzl-n-45sDUpNBpTBuQYIvEdlxRnqGgCLcBGAs/s320/DX9KVEaX0AAhZ0N.jpg"&gt;
&lt;img alt="" src="https://3.bp.blogspot.com/-idbpUQxods4/WrL7tfBPrAI/AAAAAAAAy3c/fRlBRkiszl03L2OHb90YJ8FuM5ZdatrQgCLcBGAs/s320/DX9KVEXX4AIpNYv.jpg"&gt;
&lt;img alt="" src="https://2.bp.blogspot.com/-AWSA-0nxm08/WrL7tniUZbI/AAAAAAAAy3k/u59ZVgzqyEUTG5nd3wBjHi51BweaH13XgCLcBGAs/s320/DX9KVEWWkAQR4hg.jpg"&gt;
&lt;img alt="" src="https://2.bp.blogspot.com/-Q5v-KXlNu1c/WrL7tkMJDFI/AAAAAAAAy3g/KPHHNDTV3xM7pXvKoEiSlNHT04gIJW3_ACLcBGAs/s320/DX9KVEXWsAImaHY.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Also, I really enjoyed reading the write-ups that some organizers and participats wrote:
- From Iva and Tsvetelina, organizers in Sofia: &lt;a href="https://www.facebook.com/evolutiontc/posts/2040798282603060"&gt;&lt;/a&gt;
- From Priyanka, a participant in Amsterdam: &lt;a href="https://www.linkedin.com/pulse/pandassprint-amsterdam-my-experiences-priyanka-ojha/"&gt;&lt;/a&gt;
- From &lt;a href="https://twitter.com/IHackPY"&gt;Himanshu&lt;/a&gt;, organiser in &lt;a href="https://twitter.com/PythonKanpur"&gt;Kanpur&lt;/a&gt;, India: &lt;a href="https://kanpurpython.wordpress.com/2018/03/15/experience-of-pandas-documentation-sprint/"&gt;&lt;/a&gt;
- Live streaming of the sprint in Shen Zhen: &lt;a href="https://www.youtube.com/watch?v=SK-sF_biP04"&gt;&lt;/a&gt;
- From Marc, participant in Toronto: &lt;a href="https://towardsdatascience.com/making-my-first-open-source-software-contribution-8ebf622be33c"&gt;https://towardsdatascience.com/making-my-first-open-source-software-contribution-8ebf622be33c&lt;/a&gt;
- From &lt;a href="https://bluekiri.com/"&gt;Bluekiri&lt;/a&gt;, sponsor in Mallorca: &lt;a href="https://medium.com/bluekiri/pandas-documentation-sprint-90f5a76c0e24"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And it's worth taking a look at this analysis on the impact on the sprint
in the pandas GitHub activity by &lt;a href="https://twitter.com/jorisvdbossche"&gt;Joris&lt;/a&gt;:
&lt;a href="https://jorisvandenbossche.github.io/blog/2018/03/13/pandas-sprint-activity/"&gt;https://jorisvandenbossche.github.io/blog/2018/03/13/pandas-sprint-activity/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;#pandasSprint aftermath&lt;/h2&gt;
&lt;p&gt;This is what I think was the aftermath of the sprint:
- A lot of hard work before the sprint by all the local organizers and core developers
- More than 200 pull requests sent, around 150 already merged
- Many people really loved the experience
- An incredible work by the pandas core development team after the sprint
- In London, our sprint after the 10th of March have long waiting list, which
  was not happening before the #pandasSprint
- Several people keeps contributing to the pandas documentation after
  sending their first contribution&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://1.bp.blogspot.com/-H5C_zl4ms8w/WrL7_nbUOkI/AAAAAAAAy3s/In1eAslaH0cgylzT__9RRYFpIGsnj8-3ACLcBGAs/s320/Screenshot%2Bat%2B2018-03-22%2B00-41-14.png"&gt;&lt;/p&gt;
&lt;p&gt;And what I think it's more important. We did a small but great step in making
sprints a popular event format in the Python community, to add the missing piece
to the numerous conferences, meetups based on talks, dojos, workshops and others.&lt;/p&gt;
&lt;p&gt;Several people asked me when is the next one. In London we are having two sprints
this week. Man AHL is hosting this great &lt;a href="http://ahl.com/hackathon"&gt;hackathon&lt;/a&gt;
in one month. I hope to see other user groups organizing sprints in the future.
And about another worldwide sprint... May be in some months we could do a PyData
Festival and have 10,000 people contributing to 20 different projects during a
whole weekend? :)&lt;/p&gt;</content><category term="pandas"></category></entry></feed>