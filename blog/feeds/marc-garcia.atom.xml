<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>datapythonista blog - Marc Garcia</title><link href="https://datapythonista.github.io/blog/" rel="alternate"></link><link href="https://datapythonista.github.io/blog/feeds/marc-garcia.atom.xml" rel="self"></link><id>https://datapythonista.github.io/blog/</id><updated>2019-12-01T00:00:00+00:00</updated><subtitle>about me</subtitle><entry><title>Successful delivery of data projects</title><link href="https://datapythonista.github.io/blog/successful-delivery-of-data-projects.html" rel="alternate"></link><published>2019-12-01T00:00:00+00:00</published><updated>2019-12-01T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2019-12-01:/blog/successful-delivery-of-data-projects.html</id><summary type="html">&lt;h1&gt;Successful delivery of data projects&lt;/h1&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/data_projects/tower_bridge.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;This week we organized a round table with people involved in the management
of data projects. Mostly data science.&lt;/p&gt;
&lt;p&gt;The idea came after the &lt;a href="https://pydata.org/london2019/schedule/presentation/61/executives-at-pydata/"&gt;Executives at PyData&lt;/a&gt;
session organized earlier this year. And discussions with few people on the
challenges when trying to deliver data …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Successful delivery of data projects&lt;/h1&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/data_projects/tower_bridge.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;This week we organized a round table with people involved in the management
of data projects. Mostly data science.&lt;/p&gt;
&lt;p&gt;The idea came after the &lt;a href="https://pydata.org/london2019/schedule/presentation/61/executives-at-pydata/"&gt;Executives at PyData&lt;/a&gt;
session organized earlier this year. And discussions with few people on the
challenges when trying to deliver data projects.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ianozsvald.com/"&gt;Ian Ozvald&lt;/a&gt;, founder of PyData London, and organizer
of the PyData session was present. He's also a trainer and consultant specialized
in the topic (one of his courses is coincidentially named &lt;em&gt;Successful data science projects&lt;/em&gt;),
so his participation was very appreciated. We also had &lt;a href="https://www.linkedin.com/in/mehmood-hassan7/"&gt;Mehmood Hassan&lt;/a&gt;
a recruiter from &lt;a href="https://www.interquestgroup.com/brands/ecom"&gt;ECOM&lt;/a&gt;, who kindly
offered to host the session in their offices, and feed us during the event. :)
Mehmood contributions were also valuable, bringing a recruiter's point of view
in different topics.&lt;/p&gt;
&lt;p&gt;The rest of attendees had different backgrounds, like lead data scientists,
project managers, and senior specialists in machine learning engineering.
They came from a diversity of industries, companies such as
&lt;a href="https://en.wikipedia.org/wiki/Lloyds_Bank"&gt;Lloyds Bank&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/Johnson_%26_Johnson"&gt;Johnson &amp;amp; Johnson&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/MercadoLibre"&gt;Mercado Libre&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/ITV_(TV_network)"&gt;ITV&lt;/a&gt; or
&lt;a href="https://en.wikipedia.org/wiki/J.P._Morgan_%26_Co."&gt;JP Morgan&lt;/a&gt;
among others.&lt;/p&gt;
&lt;p&gt;Next there is a summary of the discussions during the event. The topics were
proposed by the same attendees, before and during the event.&lt;/p&gt;
&lt;h2&gt;Data teams&lt;/h2&gt;
&lt;p&gt;The first topic that was discussed was about the teams. One of the first
challenges is the current terminology. Data science roles are very broad
and can mean a variety of things depending on the company. While some
progress has been made (new roles like machine learning engineer), this is
still a challenge today.&lt;/p&gt;
&lt;p&gt;There is a preference in many companies to try to hire people with an
unreasonable set of skills. From excellent software engineering skills,
to advanced knowledge of machine learning techniques, and great soft
skills. There is also a preference of companies to hire only senior
people, which makes things challenging in both sides (for candidates
and for companies).&lt;/p&gt;
&lt;p&gt;One of the companies was using a (probably) rare role, that was a team
of experts with the goal of advising and auditing practices in the
data projects. Deciding on the feasibility of projects being considered,
the recruitment, the skills needed. It was described as similar to
project management, but with very high technical knowledge.&lt;/p&gt;
&lt;p&gt;Having multiple skills in a team is also tricky. Projects that had only
data scientists doing everything (like productionizing) went wrong,
based on attendees experience. But there were different cases were
having separate teams (data scientists and devops for example) went
also wrong, because of the cultural gap. In some cases the development
ended up being more adversarial among them, than collaborative.&lt;/p&gt;
&lt;p&gt;Regarding the leadership of the projects, many options seem to be
happening. Data science teams that take ownership of the projects,
and lead the development. Meaning that other teams (data engineering,
devops) report to them. Other cases the software development team
owns the project, and data scientists report to them. There are also
cases where the leadership is shared between technical and business
managers.&lt;/p&gt;
&lt;p&gt;Participants have also shared accounts of spending quality time with
stakeholders not directly involved in the data work, like executives
or internal clients. Time spent on introducing concepts, plans, and 
getting feedback in an informal setting contributed to long term success,
i.e. made getting buy-in easier.&lt;/p&gt;
&lt;h2&gt;Organization&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/data_projects/session1.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;For organizing the teams, the preference was to use Agile techniques as
a reference, but everybody agreed that using the same techniques used
for software engineering wasn't useful. Kanban, Scrum... were adapted
and in some cases the organization was just slightly inspired by them.&lt;/p&gt;
&lt;p&gt;One of the variants was to use longer sprints than usual (assuming it's
usual to have two week sprints). The sprints were four weeks instead,
and they were divided in different stages. For example, the first
stage was analyzing the data and planning the next stages of the sprint.&lt;/p&gt;
&lt;p&gt;Another variant was to use Scrum managing the uncertainty of data projects
by making hypothesis and assumptions. The length of the tasks, the quality
of the data... are estimated or assumed, the sprint is planned for that,
and for every wrong assumption and extra work, new tasks are added to
the backlog. This was successful for one team, but required the people
in the team to be senior and experienced.&lt;/p&gt;
&lt;p&gt;The main goals by using these techniques, besides of course delivering
the expected results, were to have visibility of the development of
the project, and to estimate deadlines.&lt;/p&gt;
&lt;p&gt;Agile techniques are often based on trying things fast, and failing
early. This often does not apply to data science projects, since they
may have a big overhead to get started. For example accessibility to
the data (regulated institutions may need long processes of approval).&lt;/p&gt;
&lt;h2&gt;Technology&lt;/h2&gt;
&lt;p&gt;All companies were mainly using the PyData stack for the projects.
This is surely biased, given that the organization of the event, and
the promotion, came from people from the PyData community.&lt;/p&gt;
&lt;p&gt;The preference was to develop the projects in shared infrastructure,
and not in the workstations. As an example, 8 Gb of RAM can be quiet
good for a workstation, but clearly insufficient for many data projects.
&lt;a href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt; was the main interface used, and as
said, having it on the servers and not locally (using Jupyter Hub)
as the preferred approach.&lt;/p&gt;
&lt;p&gt;Some of the mentioned tools were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fasttext.cc/"&gt;fastText&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And for productionizing, tools like Docker or Flask (creating
microservices) were named.&lt;/p&gt;
&lt;p&gt;There was mostly agreement on building "from scratch" the platform using
these tools, and not using data frameworks with GUI...
&lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html"&gt;SageMaker&lt;/a&gt;
was used in some cases.&lt;/p&gt;
&lt;p&gt;Persistence of models is mainly done with Python's pickles.&lt;/p&gt;
&lt;h2&gt;Open source&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/data_projects/session2.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;There was also a discussion on how companies see and get involved in
open source.&lt;/p&gt;
&lt;p&gt;There were different points of views regarding hiring of open source
contributors. People closer to the recruitment side thought that it's
a clear competitive advantage to contribute to open source. While
people closer to open source development didn't see any impact of their
contributions in their professional careers.&lt;/p&gt;
&lt;p&gt;There were also concerns on whether companies care about candidates
with good GitHub profiles, because they expect them to work longer
hours (since they are used to work for free), and be more motivated.&lt;/p&gt;
&lt;p&gt;There were no experiences on companies supporting open source by hiring
contributors to work on the projects as part of their job.&lt;/p&gt;
&lt;p&gt;The experiences on companies sponsoring events were very positive.
Most companies who host events, sponsor conferences seem to see
value when they have experience, and they keep recurring. Some
have seen the quality of applications to their companies increase
significantly, more than the quantity. And candidates usually
have a very positive attitude when interviewed by companies they
know because they sponsor events.&lt;/p&gt;
&lt;p&gt;On the recruitment side, it was also side about the poor experience
of many companies, who have unreasonable tests to asses candidates.
There was agreement that tests should have a reasonable duration,
let the candidate learn things (about the company and the role
besides technical things), and useful feedback by senior technical
people should be given after it's reviewed.&lt;/p&gt;
&lt;h2&gt;The future&lt;/h2&gt;
&lt;p&gt;Regarding the future of the data industry, everybody was optimistic
and thought that it will continue to grow, even with the difficulties
in delivering results. Companies seem to keep growing their awareness
of the value of the data they have.&lt;/p&gt;
&lt;p&gt;Some areas that people expect to grow are fairness in machine learning
and interpretability.&lt;/p&gt;
&lt;p&gt;For the future of our round tables, people found the session useful.
Besides sharing experiences, people found very useful to know that
other companies have many of the same problems, and that they didn't
take a wrong path, and are disconnected from the rest of the industry.&lt;/p&gt;
&lt;p&gt;We are likely to organize sessions like this in the future. Several
people also shown interest on having them online, since they were
not based in London.&lt;/p&gt;
&lt;p&gt;If you are managing data projects, and can be interested in future
session, feel free to get in touch. You have many ways to get in
touch with me at my &lt;a href="https://datapythonista.me"&gt;website&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>An update on the pandas documentation</title><link href="https://datapythonista.github.io/blog/pandas-documentation-update.html" rel="alternate"></link><published>2019-11-28T00:00:00+00:00</published><updated>2019-11-28T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2019-11-28:/blog/pandas-documentation-update.html</id><summary type="html">&lt;h1&gt;An update on the pandas documentation&lt;/h1&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/panda_book.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;Some context&lt;/h2&gt;
&lt;p&gt;This post is mainly a technical post on what's the status of the pandas documentation.
But let me provide a bit of context on where this comes from.&lt;/p&gt;
&lt;p&gt;It's a personal opinion, but I think pandas is one of the clearest examples …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;An update on the pandas documentation&lt;/h1&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/panda_book.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;Some context&lt;/h2&gt;
&lt;p&gt;This post is mainly a technical post on what's the status of the pandas documentation.
But let me provide a bit of context on where this comes from.&lt;/p&gt;
&lt;p&gt;It's a personal opinion, but I think pandas is one of the clearest examples of how open
source is transforming the data world (together with some other projects like scikit-learn,
Jupyter, R...). Is likely that pandas contributed to the huge growth of
Python, a language that not many people knew about when pandas development started.&lt;/p&gt;
&lt;p&gt;And I think its documentation has been the clearest example of the paradox of open
source software. While pandas users were growing to the millions, and it was adopted in thousands
of companies (including the largest companies in the world), almost nobody spent any time
or money in its documentation (which requires a lot of work to be of high quality). It's surely not to blame the
very few people (3 or 4 at times) who were maintaining the project. They had enough dealing with thousands of
issues, updates in the many and fast changing dependencies, releasing new versions... While also
implementing new features and fixing bugs themselves. And even more considering that most of that
work was done as volunteers, in evenings after work or in weekends.&lt;/p&gt;
&lt;p&gt;More than 2 years ago I sent to the project my first pull request, fixing a single docstring
(one of the around 2,000 in the project). And decided to spend a significant amount of
my also volunteer time (after work and during weekends) in improving that part. That also
was one of the main reasons for starting the &lt;a href="https://python-sprints.github.io"&gt;Python sprints&lt;/a&gt;
group.&lt;/p&gt;
&lt;p&gt;More than two years later, not much changed (apparently). But if you care about my opinion,
I'm sure very soon pandas will have one of the best documentations of any open source project. This
post explains all the work done by hundreds of people in the last couple of years, and the work
that is still missing, and how we are going to approach it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you work for a company that is making money using pandas, and that would be more productive
and make even more money if pandas documentation was better, please contact a pandas maintainer
including &lt;a href="mailto:garcia.marc@gmail.com"&gt;myself&lt;/a&gt; or &lt;a href="https://www.numfocus.org"&gt;NumFOCUS&lt;/a&gt;.
We are happy to discuss funding opportunities, including small grants or helping your company
hire people to work on pandas.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The problem with the docstrings&lt;/h2&gt;
&lt;p&gt;The pandas API is huge, and includes around 2,000 functions, methods, classes, attributes...
Each of them with a page in the documentation.
Given the very reduced number of developers pandas had, and the huge demand and urgency for a dataframe
library in Python, most of those API pages couldn't be created with high standards when the
features were implemented. See for example the
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.resample.Resampler.last.html"&gt;Resampler.last&lt;/a&gt;
page.&lt;/p&gt;
&lt;p&gt;It may not be obvious for people who haven't contributed to a project like pandas before,
but improving a docstring, and make it as useful for readers as possible, it's not 5 minutes
of work. Based on the work done by lots of contributors over the last year, I would estimate
it takes around 1 day of work of an experienced pandas user. You can see the docstring of
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.resample.Resampler.bfill.html"&gt;Resampler.bfill&lt;/a&gt;
to see what I'd consider a good docstring.&lt;/p&gt;
&lt;p&gt;Considering this estimate of 1 day per docstring, and the around 2,000 docstrings, it's
easy to calculate that it'd take around 8 years for a single person working full time,
to have all them fixed. And that excludes the time of maintainers to review the changes,
provide feedback, merge...&lt;/p&gt;
&lt;h2&gt;The pandas documentation sprint&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/pandas_sprint.png"&gt;&lt;/p&gt;
&lt;p&gt;As it's obvious that a single person can't do much, one of the things that was tried
was to organize a &lt;a href="https://python-sprints.github.io/pandas/"&gt;worldwide pandas sprint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The sprint was extremely successful in many ways. 30 local users groups participated,
from places as diverse as Korea, Hong Kong, India, Turkey, Kenya, Nigeria, Argentina
or Brazil, besides many cities in Europe and US. Difficult to say how many people
participated, but we estimate there where around 500 people helping make pandas
documentation for a whole Saturday.&lt;/p&gt;
&lt;p&gt;I think there are no words to describe how amazing that is. How many people offered
to organize sprints with their local groups. And how many people joined them in every
sprint. The organizers had to prepare the event for weeks, both for logistics but also
for the technical part of leading a lot of people in their first open source contributing.&lt;/p&gt;
&lt;p&gt;A big responsible for this success was &lt;a href="https://numfocus.org/"&gt;NumFOCUS&lt;/a&gt;. For several
years now NumFOCUS has done extraordinary efforts on building the PyData community.
Having a connected network of more than 150 user groups made it very easy to communicate
and reach all the people who could be interested.&lt;/p&gt;
&lt;p&gt;The feedback I received from the participants and organizers was very positive, and people
enjoyed the experience (which I personally think it's much more important than the
contributions made). And there were around 200 documentation pages that were fixed
because of the sprint (10% of the total).&lt;/p&gt;
&lt;p&gt;But not everything was so positive. The sprint also made evident that the problem of
fixing the documentation was not the number of contributors. The bottleneck happened
to be the maintainers. The sprint created a total disruption of the project for more than two
weeks. And this period wasn't longer because the maintainers were spending day and night
reviewing the pull requests from the sprint. In many cases it was the maintainers who
had to finish the work started during the sprint. Many pull requests contained great
work, but were discontinued, and they required important changes that had to be done
by the maintainers. The last PR from the sprint was merged almost a year later than
the sprint.&lt;/p&gt;
&lt;p&gt;More information about the sprint can be found in this
&lt;a href="https://numfocus.org/blog/worldwide-pandas-sprint"&gt;NumFOCUS write up&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;The validation script&lt;/h2&gt;
&lt;p&gt;We anticipated before the sprint, that reviewing the contributions would be a lot
of work. And we developed a script to automate part of it. The idea was to automatically
detect things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The documented parameters of a function need to match the actual parameters in the signature&lt;/li&gt;
&lt;li&gt;Conventions like starting sentences with a capital letter or finishing with a period&lt;/li&gt;
&lt;li&gt;Ensure that some sections we'd like to always have are present (like Examples)&lt;/li&gt;
&lt;li&gt;Formats that make Sphinx render the documentation incorrectly, like missing spaces before the
  colon between a parameter name and its type&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We managed to have several of these ready for the sprint. But to keep all the docstrings consistent
and rendering correctly, we needed many more. Many people spent a significant amount of time adding
new checks to the script, and we are currently able to automatically detect
&lt;a href="https://github.com/pandas-dev/pandas/blob/master/scripts/validate_docstrings.py#L63"&gt;more than 40 possible problems in docstrings&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course not everything can be validated, think of correct grammar, wrong, inaccurate or misleading
information, examples that are not clear... But validating almost every formatting issue automatically
will definitely save a lot of time of reviewers. Who will be able to focus on the things that
can't be automated.&lt;/p&gt;
&lt;p&gt;Some other projects had interest in using our validation script, so it's been recently moved to
&lt;a href="https://github.com/numpy/numpydoc"&gt;numpydoc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/numpydoc_validation.png"&gt;&lt;/p&gt;
&lt;h2&gt;Continuous Integration&lt;/h2&gt;
&lt;p&gt;During the sprint, we provided clear instructions, and we had mentors in each of the 30 different
locations. So, everybody was probably aware that the validation script was one of the main things
they had to check. But regular contributors don't get this sort of induction. So, ideally we would
like to run the checks automatically in the CI, so contributors are aware of any problem in their
proposed changes. But there are some things to consider.&lt;/p&gt;
&lt;p&gt;With Travis, our main CI system, the errors ended up in a huge log, that only experienced
pandas developers are able to understand. See for example &lt;a href="https://travis-ci.org/pandas-dev/pandas/jobs/484898115"&gt;this job&lt;/a&gt;
and make sure you wait until it's fully loaded. :)&lt;/p&gt;
&lt;p&gt;Luckily, at the time our script was being implemented, Numba set up Azure Pipelines for their CI, and we
decided to use it to complement Travis. The main reason was that we required more computational
power for the huge test set of pandas, and the large number of builds. But, I think the clarity
on how the errors can be reported, is as convenient as the 30 concurrent jobs the Azure team
offered us. Compare this with the previous Travis log:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://user-images.githubusercontent.com/10058240/47961709-1ca90800-e008-11e8-80d7-cccc2c2e5776.png"&gt;&lt;/p&gt;
&lt;p&gt;We were very ambitious, and somehow pioneers in what we were doing, and it wasn't
easy to get what we wanted. But The Azure team was extremely helpful, and the final
presentation of errors in docstrings, as well as the linting errors and others was
much clearer. And friendly for first time contributors.&lt;/p&gt;
&lt;p&gt;Azure pipelines was an improvement, but we still had some challenges. The integration
with GitHub wasn't great, and it required following two links and navigating a somehow
confusing interface to arrive to the page where the errors were being presented in a
better interface than TravisCI.&lt;/p&gt;
&lt;p&gt;Just few days ago we moved this docstring validation, among other things, to
&lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt;. So far the experience has been
great, and we keep the same advantages as with Pipelines, and also we have a better
integration of the CI with GitHub. And the configuration is also simpler and a bit
more intuitive than with Pipelines.&lt;/p&gt;
&lt;p&gt;We are still figuring out how to notify contributors with a human-readable message
about problems in the CI. Since only experienced contributors know how to find the
problems by themselves. And that's taking time from the reviewers, for a task that
could be easily automated. With GitHub Actions is very easy to write comments in
a pull request, so the only remaining challenge is to obtain all the failures in
all the jobs in a run. But hopefully that's not too complex and we can have it in
place soon.&lt;/p&gt;
&lt;p&gt;Another important thing that the continuos integration can do for us, is to let
us visualize the documentation before being merged. In many cases, small details
make the documentation render in an incorrect way. Like a bold style that doesn't
finish when it's expected to finish. Or a broken table that renders as plain text.&lt;/p&gt;
&lt;p&gt;This has been challenging until now, since CI systems don't do this automatically,
and it's tricky to implement manually. CircleCI provided this feature, but to
access the rendered documentation you had to know the url and replace the id of
your build on it. Maintainers could do that in a not very efficient way, but
first-time contributors couldn't guess.&lt;/p&gt;
&lt;p&gt;Implementing it manually has a main challenge. The CI is usually only able to
access passwords and keys when a pull request is merged. Otherwise a malicious
pull request could access the key and leak it. An option to overcome this limitation
would be to host in a server a GitHub application that receives a webhook when
a pull request is opened or updated, and its docs are built. After being triggered
it would fetch the docs, and publish them somewhere, using a key only accessible
in the server.&lt;/p&gt;
&lt;p&gt;Implementing that was in our backlog for a while. But there is another problem.
Where to host all these copies of the documentation. We use GitHub pages to
host the latest version of our docs. But since Git/GitHub track all the history
of changes, the repo would grow huge very quickly if we add all our docs (around
50Mb) for every pull request.&lt;/p&gt;
&lt;p&gt;The solution to this came easier than expected, since GitHub Actions is able to
access "secrets" from pull request builds. These secrets are private values added
to a repo, and can be used to store our hosting key. And &lt;a href="https://www.ovh.co.uk/"&gt;OVH&lt;/a&gt;
has recently sponsored a new pandas cloud hosting, were we can temporary store
all these versions of the docs. This is being implemented now, and with some
luck should be in production in just couple of days. GitHub Actions will also
make very simple to delete the docs for a PR once it is merged or closed.&lt;/p&gt;
&lt;h2&gt;Validating docstrings in the CI&lt;/h2&gt;
&lt;p&gt;Two key pieces to validate contributions to the pandas docstrings are in place:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A validation script with lots of checks&lt;/li&gt;
&lt;li&gt;A CI system friendly with first time contributors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But there is a last piece needed. If we activate the validation in the CI, we will
have 1,000 docstrings, or more, reporting errors in the CI for every PR. Basically,
we will be reported of every error in every docstring that needs to be fixed every
time the CI runs.&lt;/p&gt;
&lt;p&gt;Would be useful to be able to get the errors being introduced, so no more errors
are added, while we fix the ones we already have. But this is not feasible (would be
too complex and too slow).&lt;/p&gt;
&lt;p&gt;This leaves us in a unfortunate position, of only being able to validate what has
already been fixed. Which is extremely useful, as we can guarantee that things don't
get worse. But it doesn't solve our problem of improving the docstrings that need it yet.&lt;/p&gt;
&lt;p&gt;So, what can be done? I think it's useful to divide the docstring checks in two
different categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pure formatting things (like having periods at the end of sentences)&lt;/li&gt;
&lt;li&gt;The ones that require knowledge of pandas and the object being documented
  (like adding examples of how to use an function, or adding an undocumented parameter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The ones in the first category are somehow easy to fix. And many can be fixes at a time
(in a single pull request), working in one after the other, without stopping much to
understand things.&lt;/p&gt;
&lt;p&gt;Once all the errors of a kind have been fixed, we can start validating that specific
error in the CI. And we guarantee that no more errors of that kind will ever happen.&lt;/p&gt;
&lt;p&gt;And what is even better. Once all those formatting errors are fixed, we can start
working on the docstrings, one at a time. Where a person spends enough time understanding
the function being documented to become an expert. To a point where it can focus on
improving the content, writing a nice summary, useful examples, and document parameters
in an accurate way. And that person won't need to become an expert in reStructuredText
formatting, because the CI warn about any issue. And the person will be able to fix
any problem without requiring time from a maintainer.&lt;/p&gt;
&lt;p&gt;We already completely fixed several of the more than 40, and we validate that the errors are
not added again:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GL03&lt;/strong&gt;: Use of double blank lines when one would be expected&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL04&lt;/strong&gt;: Private classes mentioned in public docstrings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL05&lt;/strong&gt;: Tabs used instead of 4 spaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL06&lt;/strong&gt;: An unknown section is found&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL07&lt;/strong&gt;: Sections in the wrong order&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL09&lt;/strong&gt;: Deprecation warning in the wrong position&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GL10&lt;/strong&gt;: Sphinx directives incorrectly formatted&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SS04&lt;/strong&gt;: Summary contains heading whitespaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SS05&lt;/strong&gt;: Summary staring with infinitive verb, not third-person&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR03&lt;/strong&gt;: Parameters are in the wrong order (compared to the signature)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR04&lt;/strong&gt;: Parameters without type&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR05&lt;/strong&gt;: Parameter type finishing with a period (it shouldn't)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR10&lt;/strong&gt;: Missing space before colon splitting parameter name and type&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RT01&lt;/strong&gt;: No Returns section&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RT04&lt;/strong&gt;: Returns description should start with a capital letter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RT05&lt;/strong&gt;: Returns description should finish with a period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA01&lt;/strong&gt;: No See Also section found&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA02&lt;/strong&gt;: See Also description should finish with a period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA03&lt;/strong&gt;: See Also description should start with a capital letter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SA05&lt;/strong&gt;: Unneeded prefix in See Also section object&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EX04&lt;/strong&gt;: pandas or NumPy explicitly imported in the examples (we assume they are always imported)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those validations have been added to the &lt;a href="https://github.com/pandas-dev/pandas/blob/master/ci/code_checks.sh#L289"&gt;code_checks.sh&lt;/a&gt;
script, which is responsible for many code quality checks (linting, typing, avoidance of unwanted patterns...).&lt;/p&gt;
&lt;p&gt;But there are still many errors that need to be fixed, including formatting errors.&lt;/p&gt;
&lt;p&gt;We are able to get the list of all the errors that we can automatically detect,
with the same script that analyzes a docstring, but without passing an object as a parameter::&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./scripts/validate_docstrings.py --format&lt;span class="o"&gt;=&lt;/span&gt;json &amp;gt; docstrings.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will output a JSON file that can easily be loaded into pandas,
and see what needs to be fixed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;docstrings.json&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orient&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;errors&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;err_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;err&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;err_list&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;barh&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/pandas_docstring_errors.png"&gt;&lt;/p&gt;
&lt;p&gt;We can see how there are 18 different validations with docstrings that don't pass them.&lt;/p&gt;
&lt;p&gt;The list of all the validations and their error codes can be found in the same
&lt;a href="https://github.com/pandas-dev/pandas/blob/master/scripts/validate_docstrings.py#L77"&gt;validation script code&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can see for example, how the error &lt;code&gt;GL08&lt;/code&gt;, happening more than 500 times, means that
the objects don't have a docstring at all. See for example
&lt;a href="http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.empty.html"&gt;Series.empty&lt;/a&gt;
Or the second most frequent error, &lt;code&gt;RT03&lt;/code&gt;
means that it is not documented what is being returned (the return type is present, but
there is no description on what's the returned object).&lt;/p&gt;
&lt;p&gt;In total we have 2,441 errors that we can detect automatically. That includes docstrings that
need to be fully created. And creating them requires a lot of time, since there is no documentation
of them, and analyzing the source code, experimenting... is required.&lt;/p&gt;
&lt;h2&gt;Where do we come from?&lt;/h2&gt;
&lt;p&gt;Analyzing the docstrings of past versions of pandas we can see that pandas 0.23 had
7,136 errors, with the current validations. pandas 0.23 was released on the 16th of
May of 2018, couple of month after the worldwide sprint with 500 people. Most of the
work of that sprint was already present in that version.&lt;/p&gt;
&lt;p&gt;This is the detail of errors at that time:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/pandas_docstring_errors_023.png"&gt;&lt;/p&gt;
&lt;p&gt;So, in 18 months we fixed two thirds of the errors. Assuming it takes the same amount
of time to fix every error, and that we continue at the same pace, in 9 months more we
should have all them fixed. This would amazing, but it's clearly false that all errors
require the same time, and it's obvious that we started by the easy ones. But we are
getting closer.&lt;/p&gt;
&lt;h2&gt;Parallelization, a game changer&lt;/h2&gt;
&lt;p&gt;Our work is every time more challenging and time consuming. Remember that we started with
small formatting changes like adding a period at the end of a sentence, and we are going 
to write full docstrings. So, we need to become smarter and more efficient on how we work on the
documentation. And the most important part is being able to work in parallel, since there is
not much a single person can do.&lt;/p&gt;
&lt;p&gt;Of the around 2,000 docstrings, around 1,400 still have errors (considering errors detected
by the script). Imagine that tomorrow we want to start working on all them. One of the main
problems would be to detect who works on what, and avoid duplicate work. Many people volunteered
in the past to help make the pandas documentation better, and there may be 1,400 people
out there who could be happy to help if they knew exactly what to do.&lt;/p&gt;
&lt;p&gt;How this is usually solved in open source is by creating issues. Then people assigned the
issue they want to work to themselves, and as people join the party, they check in the
list of unassigned issues what else need to be done.&lt;/p&gt;
&lt;p&gt;Unfortunately, a workflow as simple as this has been problematic in GitHub. While GitHub
provides a nice interface for issues, and implements the functionality of assignees, it's
limited to members of the organization (core developers). While this is useful for small
corporate projects, assignees is unused in pandas, and people have been assigning them
issues by mentioning their interest in a comment. Which it kind of works, but it doesn't
let people find unassigned issues in an easy way.&lt;/p&gt;
&lt;p&gt;Recently, we found a hack in pandas, that people is starting to use. With GitHub actions,
we are able to assign issues to people who adds a comment with the exact word `take.
While this is far from optimal, since most users won't be aware of this, it can be very
useful when organizing sprints or coordinated events, where we can communicate with
contributors. We can also write in the issue description about this new feature, so
creating a batch of issues automatically, that we expect people to assign to themselves
seems feasible.&lt;/p&gt;
&lt;p&gt;That's a game changer in parallelizing the work, since it makes the logistics much
simpler in coordinating the contributors. See more information about this new
workflow in this &lt;a href="https://datapythonista.github.io/blog/new-pandas-workflow.html"&gt;recent blog post&lt;/a&gt;
I wrote.&lt;/p&gt;
&lt;p&gt;But there are still some challenges:&lt;/p&gt;
&lt;p&gt;In many cases, pandas has templates for a docstring, that are reused in more
than one object. So different objects can use the same docstring, and it's not
easy to identify which. Creating issues automatically for each object can still
cause overlap in the work.&lt;/p&gt;
&lt;p&gt;While many things can be automated, the main bottleneck are still the reviews of
the changes. There are few people who has the knowledge and experience in pandas
to provide feedback on whether the changes to the documentation are reasonable.
And who has the permissions to merge changes into pandas. And almost all them
are volunteer, and need to take care of other obligations like full time jobs
and family.&lt;/p&gt;
&lt;h2&gt;The future&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/pandas_doc/panda_babies.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Exciting times are coming for the pandas documentation. Not only the content, but
also a whole new theme is being implemented. So, soon the documentation will
look much better.&lt;/p&gt;
&lt;p&gt;The CI is getting closer to a level where we can automate as much as possible,
and be very efficient in coordinating the documentation efforts. As well as
let first-time contributors be quite autonomous, and progress independently on
their work until a maintainer can really add value.&lt;/p&gt;
&lt;p&gt;Not only the number of maintainers in the project has been growing significantly
in the last couple of years, but a new role of triaggers has been implemented,
which can be useful in this effor.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://numfocus.org/"&gt;NumFOCUS&lt;/a&gt; has recently awarded us a small development
grant to work on the documentation. And not only will help with the documentation
but hopefully will address a problem as big as it, diversity. A group of people
from groups underrepresented as pandas contributors will be helping with the
documentation, and organizing sprints in different locations to help and
encourage more people to contribute to it.&lt;/p&gt;
&lt;p&gt;Increasing the diversity of the background of the contributors, will not only
help with the quantity of the documentation pages, but also its quantity.
pandas should be useful for a wide variety of people. If the documentation is
made and reviewed by people from different backgrounds (academic backgrounds,
geography, gender...) it will be clearer and more useful to more people, and
will better accomplish its goal.&lt;/p&gt;
&lt;p&gt;The sprints are likely to be in Jakarta, Cairo and Berlin. If you can increase
the diversity of the pandas contributors and want to participate, feel free
to contact me for more details and updates.&lt;/p&gt;
&lt;p&gt;If you would like to organize a pandas documentation sprint for minorities in
a different location, please also get in touch. We are unlikely to be able to
provide funds, but we can help you with everything else.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And if your company would benefit from a better pandas documentation, please
consider supporting the project. From funding to the project, to funding of
specific developments. And also you can consider hosting an event in your
office, letting your employees spend part of their time working on pandas,
providing in-kind donations, or anything you can think of. Please message
me or any other maintainer to discuss about opportunities.&lt;/strong&gt;&lt;/p&gt;</content><category term="pandas"></category></entry><entry><title>New pandas workflow</title><link href="https://datapythonista.github.io/blog/new-pandas-workflow.html" rel="alternate"></link><published>2019-11-17T00:00:00+00:00</published><updated>2019-11-17T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2019-11-17:/blog/new-pandas-workflow.html</id><summary type="html">&lt;p&gt;Some exciting news. After some years of organizing &lt;a href="https://python-sprints.github.io/"&gt;sprints&lt;/a&gt;,
and maintaining open source, I've been thinking on a more efficient workflow for projects with
high volume of activity, like &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An exaggerated example would be that I want to create 1,600 issues in pandas. One for each
docstring of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Some exciting news. After some years of organizing &lt;a href="https://python-sprints.github.io/"&gt;sprints&lt;/a&gt;,
and maintaining open source, I've been thinking on a more efficient workflow for projects with
high volume of activity, like &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An exaggerated example would be that I want to create 1,600 issues in pandas. One for each
docstring of the project, with the flaws that we are able to automatically detect. As a
side know, most of our validations to detect incorrect things in docstrings based on the
&lt;a href="https://numpydoc.readthedocs.io/en/latest/format.html"&gt;numpydoc standard&lt;/a&gt; are now available
in &lt;code&gt;numpydoc&lt;/code&gt; (in master). You can check the
&lt;a href="https://numpydoc.readthedocs.io/en/latest/validation.html"&gt;documentation&lt;/a&gt; to
see how to use it. And the &lt;a href="https://github.com/numpy/numpydoc/blob/master/numpydoc/validate.py#L35"&gt;source code&lt;/a&gt;
for the list of errors we validate.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/numpydoc_validation.png"&gt;&lt;/p&gt;
&lt;p&gt;Back to the example, &lt;a href="https://developer.github.com/v3/"&gt;GitHub API&lt;/a&gt; and our validations
scripts would make it very easy to create those 1,600 GitHub issues. We could create a
label &lt;code&gt;Docstring errors&lt;/code&gt; to identify them, and ask the community for help to fix
those. The community responded extremely well in the past when we ask them for help.
500 people joined our &lt;a href="https://numfocus.org/blog/worldwide-pandas-sprint"&gt;worldwide documentation sprint&lt;/a&gt;.
So, things seem feasible so far.&lt;/p&gt;
&lt;p&gt;There are just two main problems to make all this work:&lt;/p&gt;
&lt;p&gt;First, there is a small number of maintainers who would have to review, give feedback, and
merge the contributions. 1,600 pull requests is surely too much for a small group
of volunteers. We are surely in a much better position now, than when 500 people
contributed in a single day (it took months to deal with all the pull requests of the sprints).
We are around 12 active maintainers, compared to 4 at that time.
And we've been improving on making our workflow more efficient, with the
CI providing every time better feedback. More accurate, and presented in a better
way, so first time contributors can detect problems in their work without much
intervention from maintainers. &lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt;
will be key in making our workflow more efficient for code reviews (things like
contributors receiving automated emails when the CI detects something that needs to
be fixed in their work).&lt;/p&gt;
&lt;p&gt;Second, how could people know which of the 1,600 issues are available, and which
are already in the works by someone else? For small projects, GitHub has an option
&lt;code&gt;Assignees&lt;/code&gt; where members of a scrum team can assign to themselves what they are
working on. But this is not possible for a project the size of pandas, since only members
of the organization are able to self-assign issues. And even if we wanted to add
every possible contributor to the pandas GitHub organization, that would be a huge
amount of work for maintainers.&lt;/p&gt;
&lt;p&gt;The best solution should come from GitHub. Adding an option so project admins can
decide whether they want to allow any GitHub user to self-assign issues in their
projects. I've been discussing this with people at GitHub, and it is something it
may be added. But not immediately.&lt;/p&gt;
&lt;p&gt;The good news is that with the help of &lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt;
is now possible to achieve the same, in a slightly trickier way. We just added
to pandas an &lt;a href="https://github.com/pandas-dev/pandas/pull/29648"&gt;action to self-assign issues&lt;/a&gt;.
How it works is by just writing a comment with the keyword &lt;code&gt;take&lt;/code&gt; to an issue.
And few seconds later, the action will assign the issue to the commenter. This
is possible because few months ago GitHub added a feature to let
&lt;a href="https://github.blog/2019-06-25-assign-issues-to-issue-commenters/"&gt;assign issues to issue commenters&lt;/a&gt;.
It is not possible even for maintainers to assign an issue to an arbitrary user.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/github_actions_assign.png"&gt;&lt;/p&gt;
&lt;p&gt;With this simple but powerful change, now a much more efficient workflow should
be possible. The workflow could consist in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;People interested in contributing to pandas start by &lt;a href="https://python-sprints.github.io/pandas/guide/pandas_setup.html"&gt;setting up the environment&lt;/a&gt;
  and learn &lt;a href="https://docs.google.com/presentation/d/1rOSYXZPyMe9KXnbVK_xbJzw_-ijxd6bIxndmvPU6L2o/edit?usp=sharing"&gt;how to make an open source contribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Then they check the list of &lt;a href="https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22+no%3Aassignee"&gt;unassigned good first issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Once they find one that they want to work on, they write a comment with the keyword &lt;code&gt;take&lt;/code&gt; on it&lt;/li&gt;
&lt;li&gt;The issue will disappear from the list of unassigned issues,
  other people won't waste time checking whether it's available or not&lt;/li&gt;
&lt;li&gt;If the person can't finally move forward (got busy, they are not interested anymore...)
  they can simply unassign themselves from the issue, and it will be in the list again&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This new workflow scales to the 1,600 issues or more. Before, potential contributors had
a list with all issues, assigned and not assigned. They had to check each individually for comments
claiming the issue, deal with ambiguity (do messages like "can I work on this?" mean you're working
on the issue?), and possibly have some discussion, before they could know if someone else is working in the issue.&lt;/p&gt;
&lt;p&gt;One obvious problem is if people self-assigning an issue, discontinuing work on it, but not
unassigning the issue. We will see how this works, but even in the worst case, unassigned
issues will still be easy to find if they exist. For the assigned ones, people can check them,
and know immediately who to ask to know if work is still going on, or progress was made.
And to ask if the original assignee is happy to hand over the issue to the new interested contributor.&lt;/p&gt;
&lt;p&gt;Implementing a bot that unassignes issues automatically after N days of inactivity could
also be an option.&lt;/p&gt;</content><category term="pandas"></category></entry><entry><title>Dataframe summit @ EuroSciPy write up</title><link href="https://datapythonista.github.io/blog/dataframe-summit-at-euroscipy.html" rel="alternate"></link><published>2019-09-11T00:00:00+01:00</published><updated>2019-09-11T00:00:00+01:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2019-09-11:/blog/dataframe-summit-at-euroscipy.html</id><summary type="html">&lt;p&gt;Last week took place in Bilbao, Spain, &lt;a href="https://www.euroscipy.org/2019/"&gt;EuroSciPy 2019&lt;/a&gt;.
This year we introduced the &lt;a href="https://www.euroscipy.org/2019/maintainers.html"&gt;maintainers track&lt;/a&gt;
a room dedicated to discussions among maintainers. The idea is similar to the 
&lt;a href="https://en.wikipedia.org/wiki/Birds_of_a_feather_(computing)"&gt;birds of a feather&lt;/a&gt; or unconference
sessions of other conferences. But focussed on open source maintainers and contributors. And
we scheduled …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week took place in Bilbao, Spain, &lt;a href="https://www.euroscipy.org/2019/"&gt;EuroSciPy 2019&lt;/a&gt;.
This year we introduced the &lt;a href="https://www.euroscipy.org/2019/maintainers.html"&gt;maintainers track&lt;/a&gt;
a room dedicated to discussions among maintainers. The idea is similar to the 
&lt;a href="https://en.wikipedia.org/wiki/Birds_of_a_feather_(computing)"&gt;birds of a feather&lt;/a&gt; or unconference
sessions of other conferences. But focussed on open source maintainers and contributors. And
we scheduled most of the sessions in advanced, to attract the interested people to join the
conference. We also had a maintainers plenary session, in which 26 maintainers of popular
open source scientific projects participated (my guess is that around 50 maintainers attended
the conference).&lt;/p&gt;
&lt;h2&gt;Dataframe summit session&lt;/h2&gt;
&lt;p&gt;One of the sessions was a 2 hours discussion on Python dataframes. 16 people attended it, around
half of them were maintainers of dataframe open source libraries. There were also pandas users
and contributors, maintainers of other projects (PyPy, pytest) and people interested in being involved.
Also the developer of a proprietary dataframe library in Python, who could also add value to the discussion.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/static/img/blog/dataframe_summit.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Those were the libraries represented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pandas-dev/pandas"&gt;pandas&lt;/a&gt;&lt;/strong&gt; Flexible and powerful data analysis / manipulation library for Python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/dask/dask"&gt;Dask&lt;/a&gt;&lt;/strong&gt; Parallel computing with task scheduling&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/vaexio/vaex"&gt;Vaex&lt;/a&gt;&lt;/strong&gt; Out-of-Core DataFrames for Python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modin-project/modin"&gt;Modin&lt;/a&gt;&lt;/strong&gt; A dataframe framework that scales the pandas API with &lt;a href="https://github.com/ray-project/ray"&gt;Ray&lt;/a&gt; and &lt;a href="https://github.com/dask/dask"&gt;Dask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/QuantStack/xframe"&gt;xframe&lt;/a&gt;&lt;/strong&gt; DataFrame library in C++&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We started by personal introductions, project introductions, and what people wanted to get out
of the session (many people already proposed topics before the event, and we defined an agenda with those).&lt;/p&gt;
&lt;h3&gt;Document the ecosystem&lt;/h3&gt;
&lt;p&gt;One of the first topics discussed was on how to let users know what is the best dataframe
tool for their job, and how the existing packages are different. The general consensus was
that the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/ecosystem.html"&gt;pandas ecosystem&lt;/a&gt; page
is the best place for it. There are already plans to improve this page (and plans and work in progress to improve
the look and feel of the pandas website and documentation).&lt;/p&gt;
&lt;h3&gt;Apache Arrow&lt;/h3&gt;
&lt;p&gt;Another topic that was discussed early was &lt;strong&gt;&lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;&lt;/strong&gt;. Arrow's mission is to
provide a common memory representation for all dataframe libraries. So, libraries don't need to reinvent the
wheel, and transferring data among packages (e.g. pandas to R) can be done without transformations or even without
copying the memory.&lt;/p&gt;
&lt;p&gt;Vaex is already using Arrow, and pandas has plans in its &lt;a href="https://pandas.pydata.org/pandas-docs/stable/development/roadmap.html"&gt;roadmap&lt;/a&gt;
to move in that direction. People were in general happy with the idea, but there were some concerns
about decisions made in Arrow (mainly contributed by Sylvain, from xframe):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache arrow C++ API and implementation not following common C++ idioms&lt;/li&gt;
&lt;li&gt;Using a monorepo (including all bindings in the same repo as Arrow)&lt;/li&gt;
&lt;li&gt;Not a clear distinction between the specification and implementation (as in for instance project Jupyter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not only related to Arrow, but it was mentioned that would be useful to have
dataframes for streaming data. A library named &lt;a href="https://github.com/finos/perspective"&gt;Perspective&lt;/a&gt;
exists, which implements something similar, and has &lt;a href="https://github.com/timkpaine/perspective-python/"&gt;Python bindings&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Interoperability&lt;/h3&gt;
&lt;p&gt;The next topic was about &lt;strong&gt;interoperability&lt;/strong&gt;. How dataframe libraries can interact among them, and
with the rest of the ecosystem. Examples can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using the same plotting backends from different dataframe libraries&lt;/li&gt;
&lt;li&gt;Passing to &lt;a href="https://scikit-learn.org/stable/index.html"&gt;scikit-learn&lt;/a&gt; pandas-like dataframe objects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There was consensus that defining a standard (and minimal) dataframe API would help. Dataframe libraries
could extend this smaller API and offer users a much bigger APIs (like pandas). But having a subset of
operations and methods would be very useful for third party libraries expecting dataframe objects.&lt;/p&gt;
&lt;p&gt;Devin from Modin is doing research at UC Berkeley on defining this API, and he's already got some
documentation he's happy to share. Modin is already implemented with this design, and while it's
one of the less mature participating projects (in Devin's words), it's user-facing layer could
potentially be reused by other projects reimplementing dataframes with a different backend. Devin
has shared the documentation for this &lt;a href="https://modin.readthedocs.io/en/latest/architecture.html#system-architecture"&gt;design&lt;/a&gt; and the &lt;a href="https://modin.readthedocs.io/en/latest/architecture.html#modin-dataframe-api"&gt;corresponding API&lt;/a&gt; on
the &lt;a href="https://modin.readthedocs.io"&gt;Modin documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It was noted that could be useful to have a common test suite, if a standard dataframe API is defined.
There was agreement that the pandas test suite is not appropriate for other packages.&lt;/p&gt;
&lt;p&gt;NumPy did something similar in &lt;a href="https://numpy.org/neps/nep-0018-array-function-protocol.html"&gt;NEP-18&lt;/a&gt;,
which can be used as reference.&lt;/p&gt;
&lt;h3&gt;Public API improvements&lt;/h3&gt;
&lt;p&gt;At the end of the session, we discussed about possible improvements to the public pandas API.
Since several of the participants reimplemented the pandas API, was a good opportunity to see
places where they found inconsistencies, or where the API was making their lives difficult
when using other approaches.&lt;/p&gt;
&lt;p&gt;Indexing was the part of pandas that other maintainers were less happy about. The way &lt;code&gt;.loc&lt;/code&gt;
behaves was one of the comments. And being forced to have a default index, and not being able
to index by other columns were other comments.&lt;/p&gt;
&lt;h3&gt;Next steps&lt;/h3&gt;
&lt;p&gt;Couple of things were discussed to keep those discussions active, and keep coordinating on
shaping the dataframes of the future.&lt;/p&gt;
&lt;p&gt;The first was to start a workgroup, or a distribution list (or discourse). The &lt;code&gt;pandas-dev&lt;/code&gt;
list wasn't used by the participants (except the pandas maintainers), and it didn't seem
to be the appropriate place.&lt;/p&gt;
&lt;p&gt;Another idea would be to organize another bigger dataframe summit in the future. It was
proposed to be hosted somewhere in the Caribbean (ok, it was me who proposed that, and
everybody else laughed, but here I leave it again). :)&lt;/p&gt;</content><category term="pandas"></category></entry><entry><title>Setting up Fedora</title><link href="https://datapythonista.github.io/blog/setting-up-fedora.html" rel="alternate"></link><published>2018-12-05T00:00:00+00:00</published><updated>2018-12-05T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2018-12-05:/blog/setting-up-fedora.html</id><summary type="html">&lt;p&gt;Today I've got my new Dell XPS (with Ubuntu preinstalled), and this is the procedure
to set it up, and get my perfect working environment. This is expected to be useful
mainly for my &lt;strong&gt;future self&lt;/strong&gt;, but sharing it here in case someone else can find
ideas or tips that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I've got my new Dell XPS (with Ubuntu preinstalled), and this is the procedure
to set it up, and get my perfect working environment. This is expected to be useful
mainly for my &lt;strong&gt;future self&lt;/strong&gt;, but sharing it here in case someone else can find
ideas or tips that are useful. Also happy to receive comments on how you do things
differently (and potentially better).&lt;/p&gt;
&lt;p&gt;My operating system of choice is &lt;a class="reference external" href="https://spins.fedoraproject.org/mate-compiz/"&gt;Fedora MATE Compiz&lt;/a&gt;,
I think GNOME 3 was a big mistake, so staying in what was GNOME 2.&lt;/p&gt;
&lt;p&gt;After downloading the ISO, I create the live USB with &lt;a class="reference external" href="https://unetbootin.github.io/"&gt;UNetbootin&lt;/a&gt;.
This works well, but it has a problem. The label of the volume is not updated, and it becomes inconsistent
with the one that GRUB loads. This will create a lot of warnings like this:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
dracut-initqueue[602]: Warning dracut-initqueue timeout - starting timeout scripts
&lt;/pre&gt;
&lt;p&gt;With couple of final warnings:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Warning: /dev/disk/by-label/Fedora-Live-WS-x86_64-29-1 does not exist
Warning: /dev/mapper/live-rw does not exist
&lt;/pre&gt;
&lt;p&gt;To fix it, we just need to know the label of our live USB (can be obtained in the rescue terminal by
calling &lt;tt class="docutils literal"&gt;blkid&lt;/tt&gt;). And then, in the GRUB menu, press &lt;cite&gt;e&lt;/cite&gt; with the &lt;cite&gt;Start Fedora Live&lt;/cite&gt; option
selected, and replace the value of &lt;cite&gt;LABEL&lt;/cite&gt; by the correct one. A &lt;cite&gt;Ctrl-x&lt;/cite&gt; will make the system
boot with the updated configuration, and should start normally. This
&lt;a class="reference external" href="https://www.youtube.com/watch?v=C3iSqmfPRxY"&gt;video&lt;/a&gt; shows the process step by step.&lt;/p&gt;
&lt;p&gt;The default configurations during the installation work well for me (using 50Gb for &lt;cite&gt;/&lt;/cite&gt;, the rest
for &lt;cite&gt;/home/&lt;/cite&gt;, and &lt;cite&gt;ext4&lt;/cite&gt; filesystem). But I encrypt &lt;cite&gt;/home/&lt;/cite&gt;, which is not enabled by default.&lt;/p&gt;
&lt;p&gt;Once the new system is installed, and running, those are the tasks I perform.&lt;/p&gt;
&lt;div class="section" id="configuration"&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;Merge both panels into one, and leave it to the bottom (removing the workspaces and Thunderbird,
which I not use)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Mouse setup: enable touchpad click, natural scrolling and increase acceleration&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Disable screensaver, and make windows be selected when mouse moves over them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Change the terminal shorcuts to change and move tabs (I got used to the KDE shortcuts and never
bothered in learning the GNOME ones)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Change the default search engine in Firefox to &lt;a class="reference external" href="https://duckduckgo.com/"&gt;DuckDuckGo&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Set up couple of aliases in &lt;cite&gt;~/.bashrc&lt;/cite&gt;: &lt;tt class="docutils literal"&gt;alias &lt;span class="pre"&gt;rgrep=&amp;quot;grep&lt;/span&gt; &lt;span class="pre"&gt;-R&amp;quot;&lt;/span&gt;&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;alias &lt;span class="pre"&gt;vi=&amp;quot;vim&amp;quot;&lt;/span&gt;&lt;/tt&gt; (which
doesn't seem to be required anymore)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Set up &lt;cite&gt;vim&lt;/cite&gt; for Python (and remove some unwanted features like folding):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
syntax on
set number
set autoindent
set expandtab
set shiftwidth=4
set tabstop=4
set nofoldenable

execute pathogen#infect()
set statusline+=%#warningmsg#
set statusline+=%{SyntasticStatuslineFlag()}
set statusline+=%*
let g:syntastic_always_populate_loc_list = 1
let g:syntastic_auto_loc_list = 0
let g:syntastic_check_on_open = 1
let g:syntastic_check_on_wq = 0
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="installing-software"&gt;
&lt;h2&gt;Installing software&lt;/h2&gt;
&lt;p&gt;Quite happy with the software that comes preinstalled with Fedora, but few things left to install.
First adding &lt;a class="reference external" href="https://rpmfusion.org"&gt;RPM Fusion&lt;/a&gt; repositories:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm
&lt;/pre&gt;
&lt;p&gt;Then updating the system:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo dnf update
&lt;/pre&gt;
&lt;p&gt;Then installing the development group:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo dnf groupinstall &amp;quot;Development Tools&amp;quot;
&lt;/pre&gt;
&lt;p&gt;Also installing all the missing packages (or not missing, but had this list for some years now):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo dnf install vim-enhanced git vlc gimp inkscape unzip
&lt;/pre&gt;
&lt;p&gt;And finally installing &lt;a class="reference external" href="https://conda.io/miniconda.html"&gt;Miniconda&lt;/a&gt;. I prefer Miniconda over
Anaconda, because I don't like to have any package in the base environment. So, in every
environment I'm sure there are the packages I'm using (and it's not falling back to the base
environment version, which can be different of the expected).&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Useful git commands</title><link href="https://datapythonista.github.io/blog/useful-git-commands.html" rel="alternate"></link><published>2018-11-08T00:00:00+00:00</published><updated>2018-11-08T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2018-11-08:/blog/useful-git-commands.html</id><summary type="html">&lt;p&gt;While &lt;cite&gt;git&lt;/cite&gt; is surely one of my favorite tools, and increases my productivity
in a sometimes unbelivable way (like when working on 3 or 5 features at the
same time), some times there are operations that can be a bit tricky.&lt;/p&gt;
&lt;p&gt;There are plenty of git tutorials and guides to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While &lt;cite&gt;git&lt;/cite&gt; is surely one of my favorite tools, and increases my productivity
in a sometimes unbelivable way (like when working on 3 or 5 features at the
same time), some times there are operations that can be a bit tricky.&lt;/p&gt;
&lt;p&gt;There are plenty of git tutorials and guides to get started and that explain
the basic concepts. This post is not one of them. If that is what you need,
you can check these great resources:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://rogerdudler.github.io/git-guide/"&gt;git - the simple guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://medium.com/girl-writes-code/git-is-a-directed-acyclic-graph-and-what-the-heck-does-that-mean-b6c8dec65059"&gt;Git is a Directed Acyclic Graph and What the Heck Does That Mean?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://think-like-a-git.net/"&gt;Think Like (a) Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://git-scm.com/doc"&gt;Official documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is another quite popular resource, that doesn't focus on explaining
the concepts, but on what to do if you get into certain cases (aka problems):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://ohshitgit.com/"&gt;Oh shit,git!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More on the style of the latter, in this post I'll explain some operations
that are somehow advanced, I don't think are well known, but I use them
frequently. So, hopefully they can be useful to others.&lt;/p&gt;
&lt;div class="section" id="i-ve-got-some-cool-changes-but-my-history-is-a-mess"&gt;
&lt;h2&gt;I've got some cool changes, but my history is a mess&lt;/h2&gt;
&lt;p&gt;There are many reasons why this can happen. The one that I encounter most
frequently is people opening a pull request, that does not only contain
the user changes (and possibly some merges from master), but instead it
contains commits from other users in the branch, as if they were part of
the pull request. I never spent the time to research what is the cause, but
this is what I usually recommend or do.&lt;/p&gt;
&lt;p&gt;Whether it is the previous case, or because of any other reason, if you have
some changes in your branch mixed with a messy git history, the easiest
way I know to go back to a state under control is:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;git fetch upstream&lt;/tt&gt;: Just updating our local repository.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;git merge upstream/master&lt;/tt&gt;: Getting anything in the latest repository
version into our branch.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;git reset &lt;span class="pre"&gt;--soft&lt;/span&gt; upstream/master&lt;/tt&gt;: This will make that the git history
in our branch is exactly as the one in master, replacing our messy history.
And it will leave in our staging area all the changes that we made, compared
to master.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;git commit &lt;span class="pre"&gt;-m&lt;/span&gt; &amp;quot;All my changes in a single commit&amp;quot;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now the history in our branch will be equivalent as if we just created
the branch from the latest version, and added a single commit with all our
changes. As usual, we shouldn't rewrite the history if someone else pulled
our commits. But if this is a local branch, or it is remote but only used
to open a pull request, that should be all right.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="i-have-changes-in-the-working-directory-and-i-want-to-change-branch"&gt;
&lt;h2&gt;I have changes in the working directory, and I want to change branch&lt;/h2&gt;
&lt;p&gt;There are also different cases for this. The simplest case (but not
common in my case) is that you are working in a branch, and want to
go to make some changes to a different one, but your current changes are
not in a state that you want to commit.&lt;/p&gt;
&lt;p&gt;The other cases (the ones that happen to me in practice) are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;You start working in some changes, and you realize that you are in the
wrong branch.&lt;/li&gt;
&lt;li&gt;You are making some last minute addition to a pull request, and before
you commit and push, the pull request is merged. So, you want to continue
the work in a new branch.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is that when you have uncommitted changes in your working
directory, and you try to change branch, you get the next error message:
&lt;cite&gt;error: Your local changes to the following files would be overwritten by
checkout&lt;/cite&gt; preventing any branch change until you commit those changes.
But committing in the current branch is not what we want.&lt;/p&gt;
&lt;p&gt;The solution in this case is &lt;tt class="docutils literal"&gt;git stash&lt;/tt&gt;. With it, the changes in the
working directory are saved into a stack, and the working directory becomes
clean.  This allows us to freely switch branches, and perform other operations.
Once we have the environment ready, and we are in the branch in which the
stacked changes belong to, then we can simply &lt;tt class="docutils literal"&gt;git stash apply&lt;/tt&gt;. We will get
the uncommitted changes back to the working directory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="i-want-to-test-or-edit-someone-else-pull-request"&gt;
&lt;h2&gt;I want to test or edit someone else pull request&lt;/h2&gt;
&lt;p&gt;This is something that mainly project maintainers do, but that can be useful
for anyone. In general, when someone opens a pull request, the changes are
reviewed, and feedback is provided, both in the GitHub (or similar)
interface. And the author, who already got the branch locally, makes changes
and run the code. But in some cases, it may be useful to get the changes of the
pull request locally, so they can be run, and edited.&lt;/p&gt;
&lt;p&gt;One example could be a stale pull request, that was opened many months ago
and that the author is not interesting in updating anymore. But it contains
code, that with few changes, would be nice to get merged.&lt;/p&gt;
&lt;p&gt;Git is a distributed system, and there is nothing in git itself that tells
which is the &amp;quot;official&amp;quot; repository, and which are forks. To interact
with other repositories from your local copy, all you need is to set a
remote, fetch the changes, and switch to their branches. This would be
done with the commands:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;git remote add &lt;span class="pre"&gt;&amp;lt;remote-name-for-user-fork&amp;gt;&lt;/span&gt; &lt;span class="pre"&gt;&amp;lt;url-to-user-fork&amp;gt;&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;git fetch &lt;span class="pre"&gt;&amp;lt;remote-name-for-user-fork&amp;gt;&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, we already have locally all the data in the repo of the author of the
pull request. Next thing is to checkout the branch used for the pull request.
This can be done with:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;git branch &lt;span class="pre"&gt;&amp;lt;branch-name&amp;gt;&lt;/span&gt; &lt;span class="pre"&gt;--track&lt;/span&gt; &lt;span class="pre"&gt;&amp;lt;remote-name-for-user-fork&amp;gt;/&amp;lt;branch-name&amp;gt;&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now we have the code in the pull request in our working directory. And we can
run or edit.&lt;/p&gt;
&lt;p&gt;GitHub has an option when creating a pull request &amp;quot;Allow edits from
maintainers&amp;quot;, that is checked by default. If the author of the pull request
left it checked, then maintainers can push to the pull request branch
after editing it locally. So, the updates are made in the same pull request,
which can be merged when it's ready.&lt;/p&gt;
&lt;p&gt;For people that are not maintainers, when the checkbox was unchecked, or when
the fork of the author does not exist anymore, pushing to &lt;cite&gt;origin&lt;/cite&gt; (your own
fork), and opening a new pull request is required.&lt;/p&gt;
&lt;p&gt;If editing other people branches is something that needs to be done often, it
is probably a good idea to use &lt;cite&gt;hub&lt;/cite&gt;, a tool from GitHub. It can be installed
with conda:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;conda install &lt;span class="pre"&gt;-c&lt;/span&gt; &lt;span class="pre"&gt;conda-forge&lt;/span&gt; hub&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And then, checking out the branch from a pull request is as simple as:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;hub checkout &lt;span class="pre"&gt;&amp;lt;github-url-of-the-pull-request&amp;gt;&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which will set up the remotes, and make the branch track the parent, so
changes can be pushed with a simple &lt;tt class="docutils literal"&gt;git push&lt;/tt&gt; given the right permissions.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Blog moved</title><link href="https://datapythonista.github.io/blog/blog-moved.html" rel="alternate"></link><published>2018-09-08T00:00:00+01:00</published><updated>2018-09-08T00:00:00+01:00</updated><author><name>Marc Garcia</name></author><id>tag:datapythonista.github.io,2018-09-08:/blog/blog-moved.html</id><summary type="html">&lt;p&gt;It's been a while since I wanted to move my blog out of blogger.&lt;/p&gt;
&lt;p&gt;Today I finally did it. :)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hello world (from Pelican)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This new blog uses Pelican, and is hosted on GitHub pages. Which will
let me create blog posts by simply using restructuredText, or
Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;You …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's been a while since I wanted to move my blog out of blogger.&lt;/p&gt;
&lt;p&gt;Today I finally did it. :)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hello world (from Pelican)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This new blog uses Pelican, and is hosted on GitHub pages. Which will
let me create blog posts by simply using restructuredText, or
Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;You can check the source code here: &lt;a class="reference external" href="https://github.com/datapythonista/datapythonista.github.io"&gt;https://github.com/datapythonista/datapythonista.github.io&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info: &lt;a class="reference external" href="https://blog.getpelican.com/"&gt;https://blog.getpelican.com/&lt;/a&gt;&lt;/p&gt;
</content></entry></feed>